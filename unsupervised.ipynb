{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Complete Guide to Unsupervised Learning Algorithms\n",
    "\n",
    "## 📚 Introduction to Unsupervised Learning\n",
    "\n",
    "**Unsupervised Learning** is a type of machine learning where we discover hidden patterns in data **without labeled examples**. Unlike supervised learning (where we have input-output pairs), unsupervised learning works with raw data to find structure, relationships, and patterns that aren't immediately obvious.\n",
    "\n",
    "### 🌟 Why is Unsupervised Learning Important?\n",
    "\n",
    "1. **Data Exploration**: Understand your data before applying supervised methods\n",
    "2. **Feature Engineering**: Create new features for better model performance\n",
    "3. **Anomaly Detection**: Find unusual patterns or outliers\n",
    "4. **Data Compression**: Reduce data size while preserving important information\n",
    "5. **Customer Segmentation**: Group customers based on behavior patterns\n",
    "\n",
    "### 🏢 Real-World Examples\n",
    "\n",
    "- **Netflix**: Grouping users with similar viewing preferences\n",
    "- **Google News**: Clustering similar news articles together\n",
    "- **Banking**: Detecting fraudulent transactions (anomaly detection)\n",
    "- **Genetics**: Finding patterns in DNA sequences\n",
    "- **Marketing**: Customer segmentation for targeted campaigns\n",
    "\n",
    "Let's dive into the most important unsupervised learning algorithms! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, load_iris, make_swiss_roll\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Means Clustering\n",
    "\n",
    "### 🧠 Intuitive Explanation\n",
    "\n",
    "Imagine you're a teacher organizing students into study groups. You want each group to have students with similar study habits and performance levels. **K-Means** is like that organizing process:\n",
    "\n",
    "1. You decide upfront how many groups (k) you want\n",
    "2. You randomly place \"group leaders\" (centroids) in the classroom\n",
    "3. Each student joins the nearest group leader\n",
    "4. Group leaders move to the center of their group\n",
    "5. Students might switch groups if they're now closer to a different leader\n",
    "6. Repeat until everyone is happy with their groups!\n",
    "\n",
    "### ⚙️ How It Works (Mechanism)\n",
    "\n",
    "K-Means uses the concept of **centroids** (cluster centers) and **Euclidean distance** to group similar data points:\n",
    "\n",
    "1. **Initialize**: Randomly place k centroids in the data space\n",
    "2. **Assignment**: Each data point joins the closest centroid (using Euclidean distance: √[(x₁-x₂)² + (y₁-y₂)²])\n",
    "3. **Update**: Move each centroid to the center (mean) of its assigned points\n",
    "4. **Repeat**: Steps 2-3 until centroids stop moving significantly\n",
    "\n",
    "The algorithm minimizes **inertia** (sum of squared distances from points to their centroid).\n",
    "\n",
    "### 📝 Pseudo Structure / Workflow\n",
    "\n",
    "```\n",
    "ALGORITHM: K-Means Clustering\n",
    "INPUT: Dataset X, number of clusters k\n",
    "OUTPUT: Cluster assignments, centroids\n",
    "\n",
    "1. INITIALIZE k centroids randomly\n",
    "2. REPEAT until convergence:\n",
    "   a. FOR each data point:\n",
    "      - Calculate distance to all centroids\n",
    "      - Assign to nearest centroid\n",
    "   b. FOR each centroid:\n",
    "      - Move to mean position of assigned points\n",
    "   c. CHECK if centroids moved significantly\n",
    "3. RETURN final clusters and centroids\n",
    "```\n",
    "\n",
    "### ✅ Use Cases\n",
    "\n",
    "• **Customer Segmentation**: Group customers by purchase behavior  \n",
    "• **Market Research**: Segment survey responses  \n",
    "• **Image Compression**: Reduce color palette  \n",
    "• **Gene Sequencing**: Group similar genetic patterns  \n",
    "• **Recommendation Systems**: Group users with similar preferences  \n",
    "• **Quality Control**: Identify defective products  \n",
    "\n",
    "### 💡 Why & When To Use\n",
    "\n",
    "**Strengths:**\n",
    "- Simple and fast\n",
    "- Works well with spherical clusters\n",
    "- Scales well to large datasets\n",
    "- Easy to implement and interpret\n",
    "\n",
    "**Limitations:**\n",
    "- Must choose k beforehand\n",
    "- Assumes spherical clusters\n",
    "- Sensitive to initialization\n",
    "- Affected by outliers\n",
    "- Struggles with varying cluster sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Code Example\n",
    "\n",
    "> **Problem**: We have customer data with annual spending and frequency of purchases. Let's use K-Means to segment customers into 3 groups for targeted marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Customer Segmentation Results:\n",
      "Total customers: 300\n",
      "Number of clusters: 3\n",
      "\n",
      "📊 Cluster Summary:\n",
      "        Annual_Spending       Purchase_Frequency      \n",
      "                   mean   std               mean   std\n",
      "Cluster                                               \n",
      "0                 -2.70  1.30               9.06  1.50\n",
      "1                 -6.89  1.53              -7.04  1.49\n",
      "2                  4.80  1.56               2.03  1.39\n"
     ]
    }
   ],
   "source": [
    "# Generate sample customer data\n",
    "np.random.seed(42)\n",
    "X_customers, _ = make_blobs(n_samples=300, centers=3, n_features=2, \n",
    "                          cluster_std=1.5, random_state=42)\n",
    "\n",
    "# Create a DataFrame for better understanding\n",
    "customer_data = pd.DataFrame(X_customers, columns=['Annual_Spending', 'Purchase_Frequency'])\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "customer_clusters = kmeans.fit_predict(X_customers)\n",
    "\n",
    "# Add cluster labels to our data\n",
    "customer_data['Cluster'] = customer_clusters\n",
    "\n",
    "print(\" Customer Segmentation Results:\")\n",
    "print(f\"Total customers: {len(customer_data)}\")\n",
    "print(f\"Number of clusters: {len(np.unique(customer_clusters))}\")\n",
    "print(\"\\n Cluster Summary:\")\n",
    "print(customer_data.groupby('Cluster').agg({\n",
    "    'Annual_Spending': ['mean', 'std'],\n",
    "    'Purchase_Frequency': ['mean', 'std']\n",
    "}).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clustering results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original data\n",
    "ax1.scatter(X_customers[:, 0], X_customers[:, 1], alpha=0.6, s=50)\n",
    "ax1.set_title(' Original Customer Data', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Annual Spending ($)')\n",
    "ax1.set_ylabel('Purchase Frequency (times/year)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Clustered data\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for i in range(3):\n",
    "    cluster_points = X_customers[customer_clusters == i]\n",
    "    ax2.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "               c=colors[i], label=f'Cluster {i}', alpha=0.6, s=50)\n",
    "\n",
    "# Plot centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax2.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           marker='x', s=300, linewidths=3, color='black', label='Centroids')\n",
    "\n",
    "ax2.set_title(' K-Means Clustering Results', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Annual Spending ($)')\n",
    "ax2.set_ylabel('Purchase Frequency (times/year)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret the clusters\n",
    "print(\"\\n Cluster Interpretation:\")\n",
    "cluster_names = ['Low Value', 'Medium Value', 'High Value']\n",
    "for i in range(3):\n",
    "    cluster_data = customer_data[customer_data['Cluster'] == i]\n",
    "    avg_spending = cluster_data['Annual_Spending'].mean()\n",
    "    avg_frequency = cluster_data['Purchase_Frequency'].mean()\n",
    "    print(f\"Cluster {i}: {len(cluster_data)} customers\")\n",
    "    print(f\"  - Average spending: ${avg_spending:.2f}\")\n",
    "    print(f\"  - Average frequency: {avg_frequency:.2f} purchases/year\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering\n",
    "\n",
    "### 🧠 Intuitive Explanation\n",
    "\n",
    "Think of **Hierarchical Clustering** like building a family tree, but in reverse! Imagine you have a big family reunion with hundreds of relatives:\n",
    "\n",
    "**Agglomerative (Bottom-up)**: \n",
    "- Start with each person as their own \"family\"\n",
    "- Find the two most similar people and group them\n",
    "- Keep combining the most similar groups until everyone is in one big family\n",
    "- You get a tree showing how everyone is related!\n",
    "\n",
    "**Divisive (Top-down)**: \n",
    "- Start with everyone in one big group\n",
    "- Keep splitting groups based on differences\n",
    "- Stop when each person is alone\n",
    "\n",
    "### ⚙️ How It Works (Mechanism)\n",
    "\n",
    "Hierarchical clustering builds a **dendrogram** (tree diagram) showing relationships between clusters:\n",
    "\n",
    "**Key Components:**\n",
    "1. **Distance Metric**: How to measure similarity (Euclidean, Manhattan, etc.)\n",
    "2. **Linkage Criteria**: How to measure distance between clusters:\n",
    "   - **Single**: Minimum distance between any two points\n",
    "   - **Complete**: Maximum distance between any two points  \n",
    "   - **Average**: Average distance between all pairs\n",
    "   - **Ward**: Minimizes variance when merging\n",
    "\n",
    "**Process (Agglomerative):**\n",
    "1. Calculate distance matrix between all points\n",
    "2. Merge closest pair of clusters\n",
    "3. Update distance matrix\n",
    "4. Repeat until one cluster remains\n",
    "\n",
    "### 📝 Pseudo Structure / Workflow\n",
    "\n",
    "```\n",
    "ALGORITHM: Agglomerative Hierarchical Clustering\n",
    "INPUT: Dataset X, linkage method\n",
    "OUTPUT: Dendrogram, cluster hierarchy\n",
    "\n",
    "1. INITIALIZE each point as its own cluster\n",
    "2. CALCULATE distance matrix between all clusters\n",
    "3. WHILE more than one cluster exists:\n",
    "   a. FIND pair of clusters with minimum distance\n",
    "   b. MERGE these clusters\n",
    "   c. UPDATE distance matrix using linkage method\n",
    "   d. RECORD merge in dendrogram\n",
    "4. RETURN complete dendrogram\n",
    "5. CUT dendrogram at desired level to get k clusters\n",
    "```\n",
    "\n",
    "### ✅ Use Cases\n",
    "\n",
    "• **Phylogenetic Analysis**: Evolutionary relationships between species  \n",
    "• **Social Network Analysis**: Community detection  \n",
    "• **Gene Expression**: Grouping genes with similar expression patterns  \n",
    "• **Document Clustering**: Organizing research papers by topic  \n",
    "• **Image Segmentation**: Grouping similar pixels  \n",
    "• **Recommendation Systems**: Finding user groups with similar preferences  \n",
    "\n",
    "### 💡 Why & When To Use\n",
    "\n",
    "**Strengths:**\n",
    "- No need to specify number of clusters beforehand\n",
    "- Produces a hierarchy of clusters (dendrogram)\n",
    "- Deterministic results (same input = same output)\n",
    "- Can find arbitrarily shaped clusters\n",
    "- Good for small to medium datasets\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally expensive O(n³)\n",
    "- Sensitive to outliers\n",
    "- Difficult to handle large datasets\n",
    "- Once merged, cannot undo (greedy approach)\n",
    "- Choice of linkage method affects results significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Code Example\n",
    "\n",
    "> **Problem**: We have data about different wine samples with their chemical properties. Let's use Hierarchical Clustering to group similar wines and create a dendrogram to understand the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the Iris dataset as a proxy for wine data (3 classes, multiple features)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m iris \u001b[38;5;241m=\u001b[39m \u001b[43mload_iris\u001b[49m()\n\u001b[0;32m      3\u001b[0m X_wine \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m      4\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mfeature_names\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_iris' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset as a proxy for wine data (3 classes, multiple features)\n",
    "iris = load_iris()\n",
    "X_wine = iris.data\n",
    "feature_names = iris.feature_names\n",
    "true_labels = iris.target\n",
    "\n",
    "# Standardize the features (important for distance-based algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n",
    "\n",
    "print(\" Wine Dataset Information:\")\n",
    "print(f\"Number of wine samples: {X_wine.shape[0]}\")\n",
    "print(f\"Number of chemical features: {X_wine.shape[1]}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Original wine types: {len(np.unique(true_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering with different linkage methods\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    # Calculate linkage matrix\n",
    "    linkage_matrix = linkage(X_wine_scaled, method=method)\n",
    "    \n",
    "    # Create dendrogram\n",
    "    dendrogram(linkage_matrix, ax=axes[idx], truncate_mode='level', p=5,\n",
    "              leaf_rotation=90, leaf_font_size=8)\n",
    "    axes[idx].set_title(f' Dendrogram - {method.capitalize()} Linkage', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Sample Index')\n",
    "    axes[idx].set_ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply hierarchical clustering and compare with K-means\n",
    "# Use Ward linkage (generally works well)\n",
    "hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "hier_clusters = hierarchical.fit_predict(X_wine_scaled)\n",
    "\n",
    "# Compare with K-means\n",
    "kmeans_wine = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_clusters = kmeans_wine.fit_predict(X_wine_scaled)\n",
    "\n",
    "# Visualize results using first two features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# True labels\n",
    "scatter1 = axes[0].scatter(X_wine[:, 0], X_wine[:, 1], c=true_labels, \n",
    "                          cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0].set_title(' True Wine Types', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(feature_names[0])\n",
    "axes[0].set_ylabel(feature_names[1])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Hierarchical clustering results\n",
    "scatter2 = axes[1].scatter(X_wine[:, 0], X_wine[:, 1], c=hier_clusters, \n",
    "                          cmap='viridis', alpha=0.7, s=50)\n",
    "axes[1].set_title(' Hierarchical Clustering', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(feature_names[0])\n",
    "axes[1].set_ylabel(feature_names[1])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# K-means clustering results\n",
    "scatter3 = axes[2].scatter(X_wine[:, 0], X_wine[:, 1], c=kmeans_clusters, \n",
    "                          cmap='viridis', alpha=0.7, s=50)\n",
    "axes[2].set_title(' K-Means Clustering', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel(feature_names[0])\n",
    "axes[2].set_ylabel(feature_names[1])\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy (how well clusters match true labels)\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "hier_accuracy = adjusted_rand_score(true_labels, hier_clusters)\n",
    "kmeans_accuracy = adjusted_rand_score(true_labels, kmeans_clusters)\n",
    "\n",
    "print(\"\\n Clustering Performance Comparison:\")\n",
    "print(f\"Hierarchical Clustering ARI Score: {hier_accuracy:.3f}\")\n",
    "print(f\"K-Means Clustering ARI Score: {kmeans_accuracy:.3f}\")\n",
    "print(\"\\n(ARI Score: 1.0 = Perfect match, 0.0 = Random assignment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "### 🧠 Intuitive Explanation\n",
    "\n",
    "Imagine you're looking at a satellite image of Earth at night, trying to identify cities. **DBSCAN** works like this:\n",
    "\n",
    "- **Dense areas** (lots of lights close together) = Cities (clusters)\n",
    "- **Sparse areas** (few scattered lights) = Countryside (noise/outliers)\n",
    "- You don't need to know how many cities exist beforehand!\n",
    "- Cities can have any shape (not just circular like K-means assumes)\n",
    "\n",
    "DBSCAN finds **dense regions** separated by **sparse regions**, making it perfect for:\n",
    "- Irregularly shaped clusters\n",
    "- Automatically determining the number of clusters\n",
    "- Identifying outliers\n",
    "\n",
    "### ⚙️ How It Works (Mechanism)\n",
    "\n",
    "DBSCAN uses two key parameters and three types of points:\n",
    "\n",
    "**Parameters:**\n",
    "- **eps (ε)**: Maximum distance between two points to be neighbors\n",
    "- **min_samples**: Minimum points needed to form a dense region\n",
    "\n",
    "**Point Types:**\n",
    "1. **Core Point**: Has at least min_samples neighbors within eps distance\n",
    "2. **Border Point**: Within eps of a core point but has < min_samples neighbors\n",
    "3. **Noise Point**: Neither core nor border (outlier)\n",
    "\n",
    "**Algorithm Process:**\n",
    "1. For each point, count neighbors within eps distance\n",
    "2. Mark core points (≥ min_samples neighbors)\n",
    "3. Form clusters by connecting core points and their neighbors\n",
    "4. Assign border points to nearby clusters\n",
    "5. Mark remaining points as noise\n",
    "\n",
    "### 📝 Pseudo Structure / Workflow\n",
    "\n",
    "```\n",
    "ALGORITHM: DBSCAN\n",
    "INPUT: Dataset X, eps, min_samples\n",
    "OUTPUT: Cluster assignments, noise points\n",
    "\n",
    "1. INITIALIZE all points as unvisited\n",
    "2. FOR each unvisited point P:\n",
    "   a. MARK P as visited\n",
    "   b. FIND all neighbors of P within eps distance\n",
    "   c. IF neighbors < min_samples:\n",
    "      - MARK P as noise (for now)\n",
    "   d. ELSE:\n",
    "      - CREATE new cluster\n",
    "      - ADD P to cluster\n",
    "      - FOR each neighbor N:\n",
    "         - IF N is unvisited: recursively expand\n",
    "         - IF N not in any cluster: add to current cluster\n",
    "3. RETURN clusters and noise points\n",
    "```\n",
    "\n",
    "### ✅ Use Cases\n",
    "\n",
    "• **Anomaly Detection**: Fraud detection, network intrusion  \n",
    "• **Image Processing**: Segmenting objects in images  \n",
    "• **Geolocation Analysis**: Finding hotspots in GPS data  \n",
    "• **Customer Behavior**: Identifying unusual purchase patterns  \n",
    "• **Bioinformatics**: Protein structure analysis  \n",
    "• **Social Media**: Detecting communities in networks  \n",
    "• **Quality Control**: Finding defective products  \n",
    "\n",
    "### 💡 Why & When To Use\n",
    "\n",
    "**Strengths:**\n",
    "- Finds clusters of arbitrary shape\n",
    "- Automatically determines number of clusters\n",
    "- Robust to outliers (marks them as noise)\n",
    "- No need to specify cluster centers\n",
    "- Works well with non-spherical data\n",
    "\n",
    "**Limitations:**\n",
    "- Sensitive to hyperparameters (eps, min_samples)\n",
    "- Struggles with clusters of different densities\n",
    "- Performance depends on distance metric choice\n",
    "- Can be sensitive to data scaling\n",
    "- May struggle with high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Code Example\n",
    "\n",
    "> **Problem**: We have GPS coordinates of crime incidents in a city. Let's use DBSCAN to identify crime hotspots (dense areas) and isolated incidents (noise), without knowing beforehand how many hotspots exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic crime data with different shaped clusters and noise\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create irregular shaped clusters (crime hotspots)\n",
    "# Hotspot 1: Dense circular area (downtown)\n",
    "cluster1 = np.random.multivariate_normal([2, 2], [[0.5, 0], [0, 0.5]], 50)\n",
    "\n",
    "# Hotspot 2: Elongated area (along a highway)\n",
    "cluster2 = np.random.multivariate_normal([8, 6], [[2, 1.5], [1.5, 1]], 40)\n",
    "\n",
    "# Hotspot 3: Small dense area (near a mall)\n",
    "cluster3 = np.random.multivariate_normal([5, 10], [[0.3, 0], [0, 0.8]], 30)\n",
    "\n",
    "# Add random noise points (isolated crimes)\n",
    "noise = np.random.uniform(0, 12, (25, 2))\n",
    "\n",
    "# Combine all data\n",
    "X_crime = np.vstack([cluster1, cluster2, cluster3, noise])\n",
    "\n",
    "print(\" Crime Data Information:\")\n",
    "print(f\"Total crime incidents: {len(X_crime)}\")\n",
    "print(f\"Expected hotspots: 3\")\n",
    "print(f\"Expected noise points: 25\")\n",
    "\n",
    "# Visualize original data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_crime[:, 0], X_crime[:, 1], alpha=0.6, s=50)\n",
    "plt.title(' Crime Incidents in City (GPS Coordinates)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN with different parameter settings\n",
    "dbscan_params = [\n",
    "    {'eps': 0.5, 'min_samples': 5},   # Strict parameters\n",
    "    {'eps': 1.0, 'min_samples': 5},   # Medium parameters\n",
    "    {'eps': 1.5, 'min_samples': 3},   # Loose parameters\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, params in enumerate(dbscan_params):\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "    crime_clusters = dbscan.fit_predict(X_crime)\n",
    "    \n",
    "    # Count clusters and noise points\n",
    "    n_clusters = len(set(crime_clusters)) - (1 if -1 in crime_clusters else 0)\n",
    "    n_noise = list(crime_clusters).count(-1)\n",
    "    \n",
    "    # Create colors for visualization\n",
    "    unique_labels = set(crime_clusters)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    # Plot results\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Noise points in black\n",
    "            col = 'black'\n",
    "            marker = 'x'\n",
    "            size = 50\n",
    "            label = f'Noise ({n_noise} points)'\n",
    "        else:\n",
    "            marker = 'o'\n",
    "            size = 60\n",
    "            label = f'Hotspot {k}'\n",
    "        \n",
    "        class_member_mask = (crime_clusters == k)\n",
    "        xy = X_crime[class_member_mask]\n",
    "        axes[idx].scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, s=size, \n",
    "                         alpha=0.7, label=label)\n",
    "    \n",
    "    axes[idx].set_title(f'🎯 DBSCAN Results\\neps={params[\"eps\"]}, min_samples={params[\"min_samples\"]}\\n'\n",
    "                       f'Clusters: {n_clusters}, Noise: {n_noise}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Longitude')\n",
    "    axes[idx].set_ylabel('Latitude')\n",
    "    axes[idx].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Parameter Impact Analysis:\")\n",
    "for idx, params in enumerate(dbscan_params):\n",
    "    dbscan = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "    crime_clusters = dbscan.fit_predict(X_crime)\n",
    "    n_clusters = len(set(crime_clusters)) - (1 if -1 in crime_clusters else 0)\n",
    "    n_noise = list(crime_clusters).count(-1)\n",
    "    \n",
    "    print(f\"Configuration {idx+1}: eps={params['eps']}, min_samples={params['min_samples']}\")\n",
    "    print(f\"  - Found {n_clusters} hotspots\")\n",
    "    print(f\"  - {n_noise} isolated incidents\")\n",
    "    print(f\"  - {len(X_crime) - n_noise} incidents in hotspots\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DBSCAN with K-means on the same data\n",
    "# Use optimal DBSCAN parameters\n",
    "dbscan_optimal = DBSCAN(eps=1.0, min_samples=5)\n",
    "dbscan_labels = dbscan_optimal.fit_predict(X_crime)\n",
    "\n",
    "# K-means with 3 clusters (we know there should be 3 hotspots)\n",
    "kmeans_crime = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_crime.fit_predict(X_crime)\n",
    "\n",
    "# Visualization comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# DBSCAN results\n",
    "unique_dbscan = set(dbscan_labels)\n",
    "colors_db = plt.cm.Spectral(np.linspace(0, 1, len(unique_dbscan)))\n",
    "\n",
    "for k, col in zip(unique_dbscan, colors_db):\n",
    "    if k == -1:\n",
    "        col = 'black'\n",
    "        marker = 'x'\n",
    "        alpha = 0.5\n",
    "        label = 'Noise'\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        alpha = 0.7\n",
    "        label = f'Hotspot {k}'\n",
    "    \n",
    "    class_mask = (dbscan_labels == k)\n",
    "    ax1.scatter(X_crime[class_mask, 0], X_crime[class_mask, 1], \n",
    "               c=[col], marker=marker, s=60, alpha=alpha, label=label)\n",
    "\n",
    "ax1.set_title(' DBSCAN: Finds Irregular Hotspots + Noise', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# K-means results\n",
    "colors_km = ['red', 'blue', 'green']\n",
    "for i in range(3):\n",
    "    cluster_mask = (kmeans_labels == i)\n",
    "    ax2.scatter(X_crime[cluster_mask, 0], X_crime[cluster_mask, 1], \n",
    "               c=colors_km[i], s=60, alpha=0.7, label=f'Cluster {i}')\n",
    "\n",
    "# Plot K-means centroids\n",
    "centroids = kmeans_crime.cluster_centers_\n",
    "ax2.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, \n",
    "           c='black', linewidths=3, label='Centroids')\n",
    "\n",
    "ax2.set_title(' K-Means: Assumes Spherical Clusters', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Longitude')\n",
    "ax2.set_ylabel('Latitude')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "n_dbscan_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_dbscan_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(\"\\n Algorithm Comparison Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DBSCAN Results:\")\n",
    "print(f\"  - Hotspots found: {n_dbscan_clusters}\")\n",
    "print(f\"  - Noise points: {n_dbscan_noise}\")\n",
    "print(f\"  - Can handle irregular shapes: ✅\")\n",
    "print(f\"  - Automatically finds clusters: ✅\")\n",
    "print()\n",
    "print(f\"K-Means Results:\")\n",
    "print(f\"  - Clusters found: 3 (predefined)\")\n",
    "print(f\"  - Noise points: 0 (assigns everything)\")\n",
    "print(f\"  - Can handle irregular shapes: ❌\")\n",
    "print(f\"  - Need to specify cluster count: ❌\")\n",
    "print()\n",
    "print(\" DBSCAN is better for this crime hotspot analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis (PCA)\n",
    "\n",
    "### 🧠 Intuitive Explanation\n",
    "\n",
    "Imagine you're a photographer trying to capture the essence of a 3D sculpture in a 2D photo. **PCA** is like finding the best angle to take that photo:\n",
    "\n",
    "- You want to capture **the most important features** while losing as little information as possible\n",
    "- You rotate your camera to find the angle that shows **maximum variation** in the sculpture\n",
    "- Some details will be lost, but you keep the **essential characteristics**\n",
    "\n",
    "**Real-world analogy**: Imagine describing people using 100 characteristics (height, weight, age, income, etc.). PCA finds that maybe just 10 \"super-characteristics\" (like \"size\", \"wealth\", \"lifestyle\") can capture most of what makes people different from each other.\n",
    "\n",
    "### ⚙️ How It Works (Mechanism)\n",
    "\n",
    "PCA finds new axes (Principal Components) that capture maximum variance in your data:\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Variance**: How spread out data points are (high variance = more information)\n",
    "- **Principal Components**: New axes ordered by importance (PC1 captures most variance)\n",
    "- **Eigenvalues**: How much variance each component captures\n",
    "- **Eigenvectors**: The direction of each principal component\n",
    "\n",
    "**Mathematical Process:**\n",
    "1. **Standardize** data (mean=0, std=1) to give equal weight to all features\n",
    "2. **Calculate covariance matrix** showing how features relate to each other\n",
    "3. **Find eigenvalues and eigenvectors** of the covariance matrix\n",
    "4. **Sort** by eigenvalues (largest first = most important components)\n",
    "5. **Transform** original data to new coordinate system\n",
    "6. **Keep top k components** that explain desired variance (e.g., 95%)\n",
    "\n",
    "### 📝 Pseudo Structure / Workflow\n",
    "\n",
    "```\n",
    "ALGORITHM: Principal Component Analysis (PCA)\n",
    "INPUT: Dataset X (n_samples × n_features)\n",
    "OUTPUT: Transformed data, principal components\n",
    "\n",
    "1. STANDARDIZE data: X_std = (X - mean) / std\n",
    "2. COMPUTE covariance matrix: C = (X_std^T × X_std) / (n-1)\n",
    "3. FIND eigenvalues λ and eigenvectors v of C\n",
    "4. SORT eigenvalues in descending order\n",
    "5. SELECT top k eigenvectors (principal components)\n",
    "6. CREATE projection matrix W from selected eigenvectors\n",
    "7. TRANSFORM data: X_pca = X_std × W\n",
    "8. RETURN transformed data and components\n",
    "```\n",
    "\n",
    "### ✅ Use Cases\n",
    "\n",
    "• **Dimensionality Reduction**: Reduce features before machine learning  \n",
    "• **Data Visualization**: Plot high-dimensional data in 2D/3D  \n",
    "• **Image Compression**: Reduce image file sizes  \n",
    "• **Face Recognition**: Extract key facial features  \n",
    "• **Gene Analysis**: Find patterns in genetic data  \n",
    "• **Stock Market**: Identify market factors  \n",
    "• **Noise Reduction**: Remove less important variations  \n",
    "\n",
    "### 💡 Why & When To Use\n",
    "\n",
    "**Strengths:**\n",
    "- Reduces computational complexity\n",
    "- Removes correlated features\n",
    "- Helps visualize high-dimensional data\n",
    "- Can reduce overfitting in ML models\n",
    "- Mathematically guaranteed to preserve maximum variance\n",
    "- Fast and deterministic\n",
    "\n",
    "**Limitations:**\n",
    "- Components are hard to interpret (linear combinations)\n",
    "- Assumes linear relationships\n",
    "- Sensitive to feature scaling\n",
    "- May lose important information\n",
    "- Not suitable for sparse data\n",
    "- Requires choosing number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Code Example\n",
    "\n",
    "> **Problem**: We have a dataset with many features about house characteristics (size, rooms, age, location factors, etc.). Let's use PCA to reduce dimensions while preserving the most important information for visualization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic high-dimensional dataset (house characteristics)\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Generate correlated features (realistic for house data)\n",
    "# Primary factors that determine house characteristics\n",
    "size_factor = np.random.normal(0, 1, n_samples)  # Overall house size\n",
    "quality_factor = np.random.normal(0, 1, n_samples)  # Build quality\n",
    "location_factor = np.random.normal(0, 1, n_samples)  # Location desirability\n",
    "\n",
    "# Create 12 house features based on underlying factors (with noise)\n",
    "house_data = {\n",
    "    'sqft': 2000 + 500 * size_factor + np.random.normal(0, 100, n_samples),\n",
    "    'bedrooms': 3 + 0.8 * size_factor + np.random.normal(0, 0.5, n_samples),\n",
    "    'bathrooms': 2 + 0.6 * size_factor + 0.3 * quality_factor + np.random.normal(0, 0.3, n_samples),\n",
    "    'garage_spaces': 2 + 0.4 * size_factor + np.random.normal(0, 0.4, n_samples),\n",
    "    'lot_size': 8000 + 2000 * size_factor + np.random.normal(0, 500, n_samples),\n",
    "    'age': 20 - 5 * quality_factor + np.random.normal(0, 5, n_samples),\n",
    "    'renovation_score': 3 + quality_factor + np.random.normal(0, 0.5, n_samples),\n",
    "    'appliance_quality': 3 + quality_factor + np.random.normal(0, 0.4, n_samples),\n",
    "    'school_rating': 7 + location_factor + np.random.normal(0, 0.5, n_samples),\n",
    "    'crime_safety': 6 + location_factor + np.random.normal(0, 0.4, n_samples),\n",
    "    'walkability': 5 + 0.7 * location_factor + np.random.normal(0, 0.6, n_samples),\n",
    "    'commute_time': 30 - 5 * location_factor + np.random.normal(0, 5, n_samples)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_houses = pd.DataFrame(house_data)\n",
    "\n",
    "# Add house categories for coloring (based on overall desirability)\n",
    "overall_score = size_factor + quality_factor + location_factor\n",
    "df_houses['house_type'] = pd.cut(overall_score, bins=3, labels=['Basic', 'Standard', 'Premium'])\n",
    "\n",
    "print(\"House Dataset Information:\")\n",
    "print(f\"Number of houses: {len(df_houses)}\")\n",
    "print(f\"Number of features: {len(df_houses.columns) - 1}\")\n",
    "print(\"\\n Feature Summary:\")\n",
    "print(df_houses.describe().round(2))\n",
    "\n",
    "print(\"\\n House Type Distribution:\")\n",
    "print(df_houses['house_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PCA\n",
    "X_houses = df_houses.drop('house_type', axis=1)\n",
    "house_types = df_houses['house_type']\n",
    "\n",
    "# Standardize features (crucial for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_houses_scaled = scaler.fit_transform(X_houses)\n",
    "\n",
    "print(\" Data Standardization:\")\n",
    "print(f\"Original data shape: {X_houses.shape}\")\n",
    "print(f\"Standardized data shape: {X_houses_scaled.shape}\")\n",
    "print(f\"Mean after scaling: {X_houses_scaled.mean(axis=0).round(3)}\")\n",
    "print(f\"Std after scaling: {X_houses_scaled.std(axis=0).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and analyze components\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_houses_scaled)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Visualization of explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Individual component variance\n",
    "ax1.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, \n",
    "        alpha=0.7, color='steelblue')\n",
    "ax1.set_title(' Variance Explained by Each Principal Component', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, v in enumerate(explained_variance_ratio):\n",
    "    ax1.text(i + 1, v + 0.01, f'{v*100:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Cumulative variance\n",
    "ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "         'bo-', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=0.95, color='red', linestyle='--', label='95% Variance')\n",
    "ax2.axhline(y=0.90, color='orange', linestyle='--', label='90% Variance')\n",
    "ax2.set_title(' Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(cumulative_variance):\n",
    "    if i % 2 == 0:  # Show every other label to avoid crowding\n",
    "        ax2.text(i + 1, v + 0.02, f'{v*100:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for different variance thresholds\n",
    "n_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"\\n PCA Analysis Results:\")\n",
    "print(f\"Original dimensions: {X_houses.shape[1]}\")\n",
    "print(f\"Components for 90% variance: {n_90} (reduction: {((X_houses.shape[1] - n_90) / X_houses.shape[1] * 100):.1f}%)\")\n",
    "print(f\"Components for 95% variance: {n_95} (reduction: {((X_houses.shape[1] - n_95) / X_houses.shape[1] * 100):.1f}%)\")\n",
    "print(f\"\\nTop 3 components explain: {cumulative_variance[2]*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what each principal component represents\n",
    "# Get component loadings (how much each original feature contributes)\n",
    "component_matrix = pca_full.components_\n",
    "feature_names = X_houses.columns\n",
    "\n",
    "# Create a heatmap of component loadings\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(component_matrix[:4], \n",
    "            xticklabels=feature_names,\n",
    "            yticklabels=[f'PC{i+1}' for i in range(4)],\n",
    "            cmap='RdBu_r', center=0, annot=True, fmt='.2f',\n",
    "            cbar_kws={'label': 'Component Loading'})\n",
    "plt.title(' Principal Component Loadings (Top 4 Components)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Original Features')\n",
    "plt.ylabel('Principal Components')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret the components\n",
    "print(\"\\n Component Interpretation:\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(4, len(component_matrix))):\n",
    "    print(f\"\\n**Principal Component {i+1}** (explains {explained_variance_ratio[i]*100:.1f}% variance):\")\n",
    "    \n",
    "    # Find features with highest absolute loadings\n",
    "    loadings = component_matrix[i]\n",
    "    feature_importance = list(zip(feature_names, loadings))\n",
    "    feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(\"  Most influential features:\")\n",
    "    for feat, loading in feature_importance[:5]:\n",
    "        direction = \"positively\" if loading > 0 else \"negatively\"\n",
    "        print(f\"    - {feat}: {loading:.3f} ({direction} correlated)\")\n",
    "    \n",
    "    # Provide interpretation\n",
    "    top_positive = [f for f, l in feature_importance if l > 0.3]\n",
    "    top_negative = [f for f, l in feature_importance if l < -0.3]\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"   Interpretation: Likely represents 'Overall House Size/Quality'\")\n",
    "    elif i == 1:\n",
    "        print(\"   Interpretation: Likely represents 'Location Desirability'\")\n",
    "    elif i == 2:\n",
    "        print(\"   Interpretation: Likely represents 'House Age vs. Quality'\")\n",
    "    else:\n",
    "        print(\"   Interpretation: Secondary factors or noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data in reduced dimensions\n",
    "# Apply PCA with different numbers of components\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_houses_scaled)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_houses_scaled)\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 2D PCA visualization\n",
    "ax1 = plt.subplot(131)\n",
    "colors = {'Basic': 'red', 'Standard': 'blue', 'Premium': 'green'}\n",
    "for house_type in colors.keys():\n",
    "    mask = house_types == house_type\n",
    "    ax1.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
    "               c=colors[house_type], label=house_type, alpha=0.7, s=50)\n",
    "\n",
    "ax1.set_title(f' 2D PCA Visualization\\n({pca_2d.explained_variance_ratio_.sum()*100:.1f}% variance explained)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax1.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 3D PCA visualization\n",
    "ax2 = plt.subplot(132, projection='3d')\n",
    "for house_type in colors.keys():\n",
    "    mask = house_types == house_type\n",
    "    ax2.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2],\n",
    "               c=colors[house_type], label=house_type, alpha=0.7, s=30)\n",
    "\n",
    "ax2.set_title(f' 3D PCA Visualization\\n({pca_3d.explained_variance_ratio_.sum()*100:.1f}% variance explained)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax2.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax2.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)')\n",
    "ax2.legend()\n",
    "\n",
    "# Comparison with original 2D projection (first 2 features)\n",
    "ax3 = plt.subplot(133)\n",
    "for house_type in colors.keys():\n",
    "    mask = house_types == house_type\n",
    "    ax3.scatter(X_houses.iloc[mask, 0], X_houses.iloc[mask, 1], \n",
    "               c=colors[house_type], label=house_type, alpha=0.7, s=50)\n",
    "\n",
    "ax3.set_title(' Original Features\\n(sqft vs bedrooms)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Square Feet')\n",
    "ax3.set_ylabel('Bedrooms')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✨ PCA Benefits Demonstrated:\")\n",
    "print(f\" Dimensionality: Reduced from {X_houses.shape[1]} to 2-3 dimensions\")\n",
    "print(f\" Information Preserved: {pca_2d.explained_variance_ratio_.sum()*100:.1f}% with 2D, {pca_3d.explained_variance_ratio_.sum()*100:.1f}% with 3D\")\n",
    "print(f\" Visualization: Can now easily visualize high-dimensional house data\")\n",
    "print(f\" Patterns: Clear separation between house types in reduced space\")\n",
    "print(f\" Efficiency: Reduced data size by {(1 - 2/X_houses.shape[1])*100:.1f}% (2D) for ML models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "### 🧠 Intuitive Explanation\n",
    "\n",
    "Imagine you have a **giant sculpture** that you need to **flatten into a painting** while preserving how all the parts relate to each other:\n",
    "\n",
    "**t-SNE** is like having a magical artist who:\n",
    "- Looks at which parts of the sculpture are **close neighbors**\n",
    "- Tries to keep those **same neighborships** in the flat painting  \n",
    "- **Stretches apart** different groups so you can see them clearly\n",
    "- **Compresses** similar groups together\n",
    "\n",
    "Unlike PCA (which finds straight-line projections), t-SNE can:\n",
    "- Handle **curved and complex relationships**\n",
    "- Create **beautiful cluster separations**\n",
    "- Reveal **hidden patterns** that linear methods miss\n",
    "\n",
    "**Key insight**: t-SNE focuses on preserving **local neighborhoods** rather than global distances!\n",
    "\n",
    "### ⚙️ How It Works (Mechanism)\n",
    "\n",
    "t-SNE uses probability distributions to maintain neighborhood relationships:\n",
    "\n",
    "**Two-Step Process:**\n",
    "\n",
    "1. **High-Dimensional Space**: \n",
    "   - Calculate similarity between all pairs using Gaussian distribution\n",
    "   - Convert similarities to probabilities (closer points = higher probability)\n",
    "   - **Perplexity** parameter controls neighborhood size\n",
    "\n",
    "2. **Low-Dimensional Space**:\n",
    "   - Use t-distribution (heavy tails) for embedding space\n",
    "   - Start with random low-D positions\n",
    "   - Gradually adjust positions to match high-D probabilities\n",
    "   - Use gradient descent to minimize **Kullback-Leibler divergence**\n",
    "\n",
    "**Key Parameters:**\n",
    "- **Perplexity**: Balance between local vs global structure (5-50 typical)\n",
    "- **Learning Rate**: How fast to adjust positions (100-1000 typical)\n",
    "- **Iterations**: Number of optimization steps (1000+ recommended)\n",
    "\n",
    "### 📝 Pseudo Structure / Workflow\n",
    "\n",
    "```\n",
    "ALGORITHM: t-SNE\n",
    "INPUT: High-dimensional data X, perplexity, learning_rate\n",
    "OUTPUT: Low-dimensional embedding Y\n",
    "\n",
    "1. COMPUTE pairwise distances in high-D space\n",
    "2. CONVERT distances to probabilities using Gaussian:\n",
    "   - p_ij = similarity between points i and j\n",
    "   - Use perplexity to determine σ (bandwidth)\n",
    "3. INITIALIZE random positions Y in low-D space\n",
    "4. FOR each iteration:\n",
    "   a. COMPUTE probabilities in low-D using t-distribution\n",
    "   b. CALCULATE gradient of KL divergence\n",
    "   c. UPDATE positions Y using gradient descent\n",
    "   d. APPLY momentum for stability\n",
    "5. RETURN final low-dimensional embedding Y\n",
    "```\n",
    "\n",
    "### ✅ Use Cases\n",
    "\n",
    "• **Data Visualization**: Explore high-dimensional datasets  \n",
    "• **Single Cell Analysis**: Visualize gene expression patterns  \n",
    "• **Image Recognition**: Visualize learned features  \n",
    "• **Natural Language Processing**: Visualize word embeddings  \n",
    "• **Customer Segmentation**: Reveal hidden customer groups  \n",
    "• **Genomics**: Visualize genetic variations  \n",
    "• **Social Network Analysis**: Visualize community structures  \n",
    "\n",
    "### 💡 Why & When To Use\n",
    "\n",
    "**Strengths:**\n",
    "- Excellent for visualization (2D/3D)\n",
    "- Reveals complex non-linear patterns\n",
    "- Great cluster separation\n",
    "- Handles curved manifolds well\n",
    "- Creates beautiful, interpretable plots\n",
    "- Works well with various data types\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally expensive O(n²)\n",
    "- Non-deterministic (different runs = different results)\n",
    "- Cannot embed new points easily\n",
    "- Global distances not preserved\n",
    "- Sensitive to hyperparameters\n",
    "- Only for visualization (not for ML pipelines)\n",
    "- Can create false clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Code Example\n",
    "\n",
    "> **Problem**: We have high-dimensional data from different smartphone user behavior patterns (apps used, usage time, preferences, etc.). Let's use t-SNE to visualize user segments that might not be obvious in the original high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic smartphone user behavior data\n",
    "np.random.seed(42)\n",
    "n_users = 400\n",
    "\n",
    "# Define user types with different behavior patterns\n",
    "user_types = {\n",
    "    'Business': {\n",
    "        'email_hours': (6, 1.5),      # High email usage\n",
    "        'social_hours': (1, 0.5),     # Low social media\n",
    "        'gaming_hours': (0.5, 0.3),   # Very low gaming\n",
    "        'productivity_apps': (8, 2),   # Many productivity apps\n",
    "        'entertainment_apps': (3, 1),  # Few entertainment apps\n",
    "        'news_hours': (2, 0.5),       # Moderate news consumption\n",
    "        'calls_per_day': (15, 5),     # Many calls\n",
    "        'battery_usage': (85, 10),    # High battery usage\n",
    "    },\n",
    "    'Social': {\n",
    "        'email_hours': (1, 0.5),      # Low email\n",
    "        'social_hours': (6, 2),       # High social media\n",
    "        'gaming_hours': (2, 1),       # Moderate gaming\n",
    "        'productivity_apps': (2, 1),   # Few productivity apps\n",
    "        'entertainment_apps': (12, 3), # Many entertainment apps\n",
    "        'news_hours': (0.5, 0.3),     # Low news\n",
    "        'calls_per_day': (8, 3),      # Moderate calls\n",
    "        'battery_usage': (75, 15),    # Moderate battery\n",
    "    },\n",
    "    'Gamer': {\n",
    "        'email_hours': (0.5, 0.3),    # Very low email\n",
    "        'social_hours': (3, 1),       # Moderate social\n",
    "        'gaming_hours': (8, 3),       # Very high gaming\n",
    "        'productivity_apps': (1, 0.5), # Very few productivity\n",
    "        'entertainment_apps': (15, 4), # Many entertainment\n",
    "        'news_hours': (0.3, 0.2),     # Very low news\n",
    "        'calls_per_day': (3, 2),      # Few calls\n",
    "        'battery_usage': (95, 8),     # Very high battery\n",
    "    },\n",
    "    'Senior': {\n",
    "        'email_hours': (2, 1),        # Moderate email\n",
    "        'social_hours': (1, 0.5),     # Low social media\n",
    "        'gaming_hours': (1, 0.5),     # Low gaming\n",
    "        'productivity_apps': (3, 1),   # Few apps overall\n",
    "        'entertainment_apps': (2, 1),  # Few entertainment\n",
    "        'news_hours': (3, 1),         # High news consumption\n",
    "        'calls_per_day': (12, 4),     # Many calls\n",
    "        'battery_usage': (40, 10),    # Low battery usage\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate data for each user type\n",
    "all_data = []\n",
    "all_labels = []\n",
    "users_per_type = n_users // len(user_types)\n",
    "\n",
    "for user_type, characteristics in user_types.items():\n",
    "    for _ in range(users_per_type):\n",
    "        user_data = []\n",
    "        for feature, (mean, std) in characteristics.items():\n",
    "            value = np.random.normal(mean, std)\n",
    "            user_data.append(max(0, value))  # Ensure non-negative values\n",
    "        \n",
    "        all_data.append(user_data)\n",
    "        all_labels.append(user_type)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_users = np.array(all_data)\n",
    "y_users = np.array(all_labels)\n",
    "feature_names = list(user_types['Business'].keys())\n",
    "\n",
    "# Create DataFrame for better understanding\n",
    "df_users = pd.DataFrame(X_users, columns=feature_names)\n",
    "df_users['user_type'] = y_users\n",
    "\n",
    "print(\" Smartphone User Behavior Dataset:\")\n",
    "print(f\"Total users: {len(df_users)}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"User types: {list(user_types.keys())}\")\n",
    "print(\"\\n User Type Distribution:\")\n",
    "print(df_users['user_type'].value_counts())\n",
    "\n",
    "print(\"\\n Feature Summary by User Type:\")\n",
    "summary = df_users.groupby('user_type')[feature_names].mean().round(2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data for t-SNE\n",
    "scaler = StandardScaler()\n",
    "X_users_scaled = scaler.fit_transform(X_users)\n",
    "\n",
    "# Compare PCA vs t-SNE visualization\n",
    "# First, apply PCA for comparison\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_users_scaled)\n",
    "\n",
    "# Apply t-SNE with different perplexity values\n",
    "perplexity_values = [5, 30, 50]\n",
    "tsne_results = {}\n",
    "\n",
    "print(\" Running t-SNE with different perplexity values...\")\n",
    "print(\"(This may take a moment - t-SNE is computationally intensive)\")\n",
    "\n",
    "for perp in perplexity_values:\n",
    "    print(f\"  Computing t-SNE with perplexity={perp}...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perp, random_state=42, \n",
    "                learning_rate=200, n_iter=1000, verbose=0)\n",
    "    tsne_results[perp] = tsne.fit_transform(X_users_scaled)\n",
    "\n",
    "print(\" t-SNE computations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA vs t-SNE results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Color mapping for user types\n",
    "color_map = {'Business': 'red', 'Social': 'blue', 'Gamer': 'green', 'Senior': 'orange'}\n",
    "colors = [color_map[label] for label in y_users]\n",
    "\n",
    "# Plot PCA results\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7, s=40)\n",
    "axes[0].set_title(f'PCA Visualization\\n({pca.explained_variance_ratio_.sum()*100:.1f}% variance explained)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot t-SNE results with different perplexity values\n",
    "for idx, perp in enumerate(perplexity_values):\n",
    "    ax_idx = idx + 1\n",
    "    X_tsne = tsne_results[perp]\n",
    "    \n",
    "    axes[ax_idx].scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.7, s=40)\n",
    "    axes[ax_idx].set_title(f't-SNE (perplexity={perp})', fontsize=12, fontweight='bold')\n",
    "    axes[ax_idx].set_xlabel('t-SNE 1')\n",
    "    axes[ax_idx].set_ylabel('t-SNE 2')\n",
    "    axes[ax_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                             markersize=10, label=user_type) \n",
    "                  for user_type, color in color_map.items()]\n",
    "fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, 0.02), ncol=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(\" PCA Results:\")\n",
    "print(\"  - Linear projection preserving maximum variance\")\n",
    "print(\"  - User types somewhat overlapped\")\n",
    "print(\"  - Global structure preserved but local patterns unclear\")\n",
    "print()\n",
    "print(\" t-SNE Results:\")\n",
    "print(\"  - Non-linear embedding preserving local neighborhoods\")\n",
    "print(\"  - Much clearer separation between user types\")\n",
    "print(\"  - Different perplexity values show different granularity:\")\n",
    "print(\"    • Low perplexity (5): Very tight, local clusters\")\n",
    "print(\"    • Medium perplexity (30): Balanced local-global structure\")\n",
    "print(\"    • High perplexity (50): More global structure preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best t-SNE result (perplexity=30)\n",
    "best_tsne = tsne_results[30]\n",
    "\n",
    "# Calculate cluster quality metrics\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Convert labels to numeric for scoring\n",
    "label_encoder = {label: idx for idx, label in enumerate(color_map.keys())}\n",
    "y_numeric = [label_encoder[label] for label in y_users]\n",
    "\n",
    "# Calculate scores for original data vs t-SNE embedding\n",
    "original_silhouette = silhouette_score(X_users_scaled, y_numeric)\n",
    "tsne_silhouette = silhouette_score(best_tsne, y_numeric)\n",
    "\n",
    "original_ch = calinski_harabasz_score(X_users_scaled, y_numeric)\n",
    "tsne_ch = calinski_harabasz_score(best_tsne, y_numeric)\n",
    "\n",
    "# Create detailed visualization of the best t-SNE result\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Scatter plot with better styling\n",
    "for user_type, color in color_map.items():\n",
    "    mask = y_users == user_type\n",
    "    ax1.scatter(best_tsne[mask, 0], best_tsne[mask, 1], \n",
    "               c=color, label=user_type, alpha=0.7, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax1.set_title(' t-SNE User Behavior Segmentation\\n(Perplexity=30)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('t-SNE Component 1')\n",
    "ax1.set_ylabel('t-SNE Component 2')\n",
    "ax1.legend(title='User Types', loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance heatmap (original vs t-SNE space correlation)\n",
    "# Calculate correlation between original features and t-SNE components\n",
    "feature_correlations = []\n",
    "for i, feature in enumerate(feature_names):\n",
    "    corr_comp1 = np.corrcoef(X_users_scaled[:, i], best_tsne[:, 0])[0, 1]\n",
    "    corr_comp2 = np.corrcoef(X_users_scaled[:, i], best_tsne[:, 1])[0, 1]\n",
    "    feature_correlations.append([corr_comp1, corr_comp2])\n",
    "\n",
    "corr_matrix = np.array(feature_correlations)\n",
    "im = ax2.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax2.set_title(' Feature Contributions to t-SNE Components', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('t-SNE Components')\n",
    "ax2.set_ylabel('Original Features')\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_xticklabels(['t-SNE 1', 't-SNE 2'])\n",
    "ax2.set_yticks(range(len(feature_names)))\n",
    "ax2.set_yticklabels([name.replace('_', ' ').title() for name in feature_names])\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(len(feature_names)):\n",
    "    for j in range(2):\n",
    "        text = ax2.text(j, i, f'{corr_matrix[i, j]:.2f}', \n",
    "                       ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax2, label='Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print analysis summary\n",
    "print(\"\\n Quantitative Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Silhouette Score:\")\n",
    "print(f\"  Original space: {original_silhouette:.3f}\")\n",
    "print(f\"  t-SNE space: {tsne_silhouette:.3f}\")\n",
    "print(f\"  Improvement: {((tsne_silhouette - original_silhouette) / original_silhouette * 100):+.1f}%\")\n",
    "print()\n",
    "print(f\"Calinski-Harabasz Score:\")\n",
    "print(f\"  Original space: {original_ch:.1f}\")\n",
    "print(f\"  t-SNE space: {tsne_ch:.1f}\")\n",
    "print(f\"  Improvement: {((tsne_ch - original_ch) / original_ch * 100):+.1f}%\")\n",
    "print()\n",
    "print(\" Key Insights:\")\n",
    "print(\"• t-SNE clearly separates the 4 user behavior types\")\n",
    "print(\"• Gaming hours and battery usage strongly influence t-SNE Component 1\")\n",
    "print(\"• Social media and productivity apps influence t-SNE Component 2\")\n",
    "print(\"• Each user type forms distinct clusters in the embedding space\")\n",
    "print(\"• Much better separation than linear PCA for this non-linear data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Autoencoders (Deep Learning Approach)\n",
    "\n",
    "### 🧠 Intuitive Explanation\n",
    "\n",
    "Imagine you have a **magic photocopier** that works in a special way:\n",
    "\n",
    "1. **Encoder**: Takes your original document and compresses it into a **tiny summary** (like creating a Twitter summary of a book)\n",
    "2. **Decoder**: Takes that tiny summary and tries to **recreate the original document**\n",
    "3. **Training**: You keep adjusting the photocopier until it gets really good at recreating documents from summaries\n",
    "\n",
    "**Autoencoder** works similarly:\n",
    "- **Input**: Original high-dimensional data (like 784 pixels in an image)\n",
    "- **Encoder**: Compresses to lower dimensions (like 64 numbers)\n",
    "- **Decoder**: Tries to reconstruct the original from the compressed version\n",
    "- **Goal**: Learn the most important features for reconstruction\n",
    "\n",
    "The magic happens in the **bottleneck** (compressed representation) - it captures the essence of your data!\n",
    "\n",
    "### ⚙️ How It Works (Mechanism)\n",
    "\n",
    "Autoencoders use neural networks to learn efficient data representations:\n",
    "\n",
    "**Architecture:**\n",
    "- **Input Layer**: Original data dimensions (e.g., 100 features)\n",
    "- **Encoder**: Gradually reduces dimensions (100 → 50 → 25 → 10)\n",
    "- **Bottleneck/Latent Space**: Smallest layer (e.g., 10 dimensions)\n",
    "- **Decoder**: Gradually increases dimensions (10 → 25 → 50 → 100)\n",
    "- **Output Layer**: Reconstructed data (same size as input)\n",
    "\n",
    "**Training Process:**\n",
    "1. **Forward Pass**: Input → Encoder → Bottleneck → Decoder → Reconstruction\n",
    "2. **Loss Calculation**: Compare reconstruction with original (MSE, BCE, etc.)\n",
    "3. **Backpropagation**: Adjust weights to minimize reconstruction error\n",
    "4. **Repeat**: Until the autoencoder learns good representations\n",
    "\n",
    "**Types of Autoencoders:**\n",
    "- **Vanilla**: Basic compression/reconstruction\n",
    "- **Sparse**: Forces most neurons to be inactive (sparse representations)\n",
    "- **Denoising**: Learns to remove noise from data\n",
    "- **Variational (VAE)**: Learns probability distributions\n",
    "\n",
    "### 📝 Pseudo Structure / Workflow\n",
    "\n",
    "```\n",
    "ALGORITHM: Autoencoder Training\n",
    "INPUT: Training data X, architecture config\n",
    "OUTPUT: Trained encoder and decoder networks\n",
    "\n",
    "1. DEFINE architecture:\n",
    "   - Encoder: [input_dim → hidden1 → hidden2 → latent_dim]\n",
    "   - Decoder: [latent_dim → hidden2 → hidden1 → input_dim]\n",
    "2. INITIALIZE network weights randomly\n",
    "3. FOR each training epoch:\n",
    "   a. FOR each batch of data:\n",
    "      - Forward pass: x → encoder → z → decoder → x_reconstructed\n",
    "      - Calculate loss: L = ||x - x_reconstructed||²\n",
    "      - Backpropagate gradients\n",
    "      - Update weights using optimizer (Adam, SGD, etc.)\n",
    "4. RETURN trained encoder (for dimensionality reduction)\n",
    "```\n",
    "\n",
    "### ✅ Use Cases\n",
    "\n",
    "• **Dimensionality Reduction**: Alternative to PCA for non-linear data  \n",
    "• **Anomaly Detection**: Identify data that doesn't reconstruct well  \n",
    "• **Image Denoising**: Remove noise from images  \n",
    "• **Data Compression**: Compress data while preserving important features  \n",
    "• **Feature Learning**: Learn representations for downstream tasks  \n",
    "• **Generative Modeling**: Generate new data similar to training data  \n",
    "• **Recommender Systems**: Learn user-item representations  \n",
    "\n",
    "### 💡 Why & When To Use\n",
    "\n",
    "**Strengths:**\n",
    "- Handles non-linear relationships (unlike PCA)\n",
    "- Flexible architecture for different data types\n",
    "- Can learn complex patterns and features\n",
    "- Useful for both supervised and unsupervised tasks\n",
    "- Can be combined with other deep learning models\n",
    "- Scalable to large datasets\n",
    "\n",
    "**Limitations:**\n",
    "- Requires large amounts of data\n",
    "- Computationally expensive to train\n",
    "- Many hyperparameters to tune\n",
    "- Can overfit on small datasets\n",
    "- \"Black box\" - difficult to interpret\n",
    "- Requires deep learning expertise\n",
    "- May not outperform simpler methods on simple data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Code Example\n",
    "\n",
    "> **Problem**: We have high-dimensional sensor data from manufacturing equipment. Let's build an autoencoder to learn compressed representations and detect anomalies (equipment malfunctions) by identifying data points that don't reconstruct well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This example uses basic neural network concepts\n",
    "# In practice, you'd use TensorFlow/PyTorch for more complex autoencoders\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Generate synthetic manufacturing sensor data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "\n",
    "# Create normal operating conditions (most of the data)\n",
    "# Simulate sensors that are correlated (realistic for manufacturing)\n",
    "normal_data = []\n",
    "\n",
    "for i in range(int(0.9 * n_samples)):\n",
    "    # Base operating conditions\n",
    "    temperature_base = np.random.normal(75, 5)  # °C\n",
    "    pressure_base = np.random.normal(100, 10)   # PSI\n",
    "    vibration_base = np.random.normal(2, 0.5)   # mm/s\n",
    "    \n",
    "    # Create 20 sensor readings with realistic correlations\n",
    "    sensor_reading = [\n",
    "        temperature_base + np.random.normal(0, 2),                    # Temp sensor 1\n",
    "        temperature_base + np.random.normal(0, 2),                    # Temp sensor 2  \n",
    "        temperature_base * 1.1 + np.random.normal(0, 3),            # Related temp sensor\n",
    "        pressure_base + np.random.normal(0, 5),                      # Pressure sensor 1\n",
    "        pressure_base + np.random.normal(0, 5),                      # Pressure sensor 2\n",
    "        pressure_base * 0.95 + np.random.normal(0, 4),              # Related pressure\n",
    "        vibration_base + np.random.normal(0, 0.3),                   # Vibration X\n",
    "        vibration_base + np.random.normal(0, 0.3),                   # Vibration Y\n",
    "        vibration_base * 1.2 + np.random.normal(0, 0.4),            # Vibration Z\n",
    "        50 + 0.3 * temperature_base + np.random.normal(0, 5),       # RPM (temp dependent)\n",
    "        30 + 0.2 * pressure_base + np.random.normal(0, 3),          # Flow rate\n",
    "        200 + temperature_base * 2 + np.random.normal(0, 20),       # Power consumption\n",
    "        np.random.normal(0.95, 0.05),                                # Efficiency\n",
    "        np.random.normal(7.2, 0.3),                                  # pH level\n",
    "        np.random.normal(45, 5),                                     # Humidity %\n",
    "        temperature_base - 10 + np.random.normal(0, 2),             # Cooling temp\n",
    "        vibration_base * 50 + np.random.normal(0, 10),              # Acoustic level\n",
    "        pressure_base / 10 + np.random.normal(0, 1),                # Secondary pressure\n",
    "        np.random.normal(12, 1),                                     # Voltage\n",
    "        np.random.normal(5, 0.5),                                    # Current\n",
    "    ]\n",
    "    normal_data.append(sensor_reading)\n",
    "\n",
    "# Create anomalous data (equipment malfunctions)\n",
    "anomaly_data = []\n",
    "anomaly_types = []\n",
    "\n",
    "for i in range(int(0.1 * n_samples)):\n",
    "    # Choose anomaly type\n",
    "    anomaly_type = np.random.choice(['overheating', 'pressure_drop', 'high_vibration', 'power_spike'])\n",
    "    anomaly_types.append(anomaly_type)\n",
    "    \n",
    "    # Start with normal base values\n",
    "    temp_base = np.random.normal(75, 5)\n",
    "    pressure_base = np.random.normal(100, 10)\n",
    "    vibration_base = np.random.normal(2, 0.5)\n",
    "    \n",
    "    # Introduce specific anomalies\n",
    "    if anomaly_type == 'overheating':\n",
    "        temp_base += 25  # Significant temperature increase\n",
    "    elif anomaly_type == 'pressure_drop':\n",
    "        pressure_base -= 30  # Pressure drop\n",
    "    elif anomaly_type == 'high_vibration':\n",
    "        vibration_base += 3  # High vibration\n",
    "    elif anomaly_type == 'power_spike':\n",
    "        temp_base += 10  # Temperature increase from power spike\n",
    "    \n",
    "    # Create sensor readings with anomaly\n",
    "    sensor_reading = [\n",
    "        temp_base + np.random.normal(0, 2),\n",
    "        temp_base + np.random.normal(0, 2),\n",
    "        temp_base * 1.1 + np.random.normal(0, 3),\n",
    "        pressure_base + np.random.normal(0, 5),\n",
    "        pressure_base + np.random.normal(0, 5),\n",
    "        pressure_base * 0.95 + np.random.normal(0, 4),\n",
    "        vibration_base + np.random.normal(0, 0.3),\n",
    "        vibration_base + np.random.normal(0, 0.3),\n",
    "        vibration_base * 1.2 + np.random.normal(0, 0.4),\n",
    "        50 + 0.3 * temp_base + np.random.normal(0, 5),\n",
    "        30 + 0.2 * pressure_base + np.random.normal(0, 3),\n",
    "        (300 if anomaly_type == 'power_spike' else 200) + temp_base * 2 + np.random.normal(0, 20),\n",
    "        np.random.normal(0.85 if anomaly_type != 'normal' else 0.95, 0.05),\n",
    "        np.random.normal(7.2, 0.3),\n",
    "        np.random.normal(45, 5),\n",
    "        temp_base - 10 + np.random.normal(0, 2),\n",
    "        vibration_base * 50 + np.random.normal(0, 10),\n",
    "        pressure_base / 10 + np.random.normal(0, 1),\n",
    "        np.random.normal(12, 1),\n",
    "        np.random.normal(5, 0.5),\n",
    "    ]\n",
    "    anomaly_data.append(sensor_reading)\n",
    "\n",
    "# Combine normal and anomalous data\n",
    "X_sensors = np.array(normal_data + anomaly_data)\n",
    "y_labels = np.array(['normal'] * len(normal_data) + ['anomaly'] * len(anomaly_data))\n",
    "\n",
    "# Create feature names\n",
    "feature_names = [\n",
    "    'temp_1', 'temp_2', 'temp_3', 'pressure_1', 'pressure_2', 'pressure_3',\n",
    "    'vibration_x', 'vibration_y', 'vibration_z', 'rpm', 'flow_rate', \n",
    "    'power_consumption', 'efficiency', 'ph_level', 'humidity', \n",
    "    'cooling_temp', 'acoustic_level', 'pressure_secondary', 'voltage', 'current'\n",
    "]\n",
    "\n",
    "print(\"🏭 Manufacturing Sensor Dataset:\")\n",
    "print(f\"Total samples: {len(X_sensors)}\")\n",
    "print(f\"Features (sensors): {len(feature_names)}\")\n",
    "print(f\"Normal samples: {np.sum(y_labels == 'normal')}\")\n",
    "print(f\"Anomaly samples: {np.sum(y_labels == 'anomaly')}\")\n",
    "print(f\"Anomaly rate: {np.mean(y_labels == 'anomaly') * 100:.1f}%\")\n",
    "\n",
    "# Display data statistics\n",
    "df_sensors = pd.DataFrame(X_sensors, columns=feature_names)\n",
    "df_sensors['label'] = y_labels\n",
    "\n",
    "print(\"\\n📊 Sensor Data Summary:\")\n",
    "summary_stats = df_sensors.groupby('label')[feature_names[:5]].agg(['mean', 'std']).round(2)\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the sensor data\n",
    "scaler = StandardScaler()\n",
    "X_sensors_scaled = scaler.fit_transform(X_sensors)\n",
    "\n",
    "# Split data: train only on normal data (realistic for anomaly detection)\n",
    "normal_mask = y_labels == 'normal'\n",
    "X_normal = X_sensors_scaled[normal_mask]\n",
    "X_test = X_sensors_scaled  # Test on all data (normal + anomalies)\n",
    "\n",
    "# Split normal data for training/validation\n",
    "X_train, X_val = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"🔄 Building Autoencoder Architecture...\")\n",
    "print(f\"Input dimension: {X_sensors.shape[1]}\")\n",
    "print(f\"Training samples (normal only): {len(X_train)}\")\n",
    "print(f\"Validation samples (normal only): {len(X_val)}\")\n",
    "print(f\"Test samples (normal + anomalies): {len(X_test)}\")\n",
    "\n",
    "# Create a simple autoencoder using MLPRegressor\n",
    "# Architecture: 20 → 10 → 5 → 10 → 20 (bottleneck dimension = 5)\n",
    "# Note: This is a simplified approach. Production autoencoders use TensorFlow/PyTorch\n",
    "\n",
    "print(\"\\n Training Autoencoder...\")\n",
    "print(\"Architecture: Input(20) → Hidden(15) → Bottleneck(8) → Hidden(15) → Output(20)\")\n",
    "\n",
    "# Since MLPRegressor doesn't directly support encoder-decoder architecture,\n",
    "# we'll train it to map input to input (identity mapping) with a bottleneck\n",
    "autoencoder = MLPRegressor(\n",
    "    hidden_layer_sizes=(15, 8, 15),  # Bottleneck architecture\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "# Train autoencoder on normal data only\n",
    "autoencoder.fit(X_train, X_train)  # Input = Output for autoencoder\n",
    "\n",
    "print(\" Autoencoder training completed!\")\n",
    "print(f\"Final training score: {autoencoder.score(X_train, X_train):.4f}\")\n",
    "print(f\"Validation score: {autoencoder.score(X_val, X_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autoencoder for anomaly detection\n",
    "# Normal data should reconstruct well (low error)\n",
    "# Anomalous data should reconstruct poorly (high error)\n",
    "\n",
    "# Get reconstructions for all test data\n",
    "X_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate reconstruction errors (MSE per sample)\n",
    "reconstruction_errors = np.mean((X_test - X_reconstructed) ** 2, axis=1)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "results_df = pd.DataFrame({\n",
    "    'reconstruction_error': reconstruction_errors,\n",
    "    'true_label': y_labels,\n",
    "    'sample_id': range(len(y_labels))\n",
    "})\n",
    "\n",
    "# Visualize reconstruction errors\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot of errors by true label\n",
    "normal_errors = results_df[results_df['true_label'] == 'normal']['reconstruction_error']\n",
    "anomaly_errors = results_df[results_df['true_label'] == 'anomaly']['reconstruction_error']\n",
    "\n",
    "ax1.boxplot([normal_errors, anomaly_errors], labels=['Normal', 'Anomaly'], patch_artist=True,\n",
    "           boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "           medianprops=dict(color='red', linewidth=2))\n",
    "ax1.set_title(' Reconstruction Error Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Reconstruction Error (MSE)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot of reconstruction errors\n",
    "colors = {'normal': 'blue', 'anomaly': 'red'}\n",
    "for label in ['normal', 'anomaly']:\n",
    "    mask = results_df['true_label'] == label\n",
    "    ax2.scatter(results_df[mask]['sample_id'], results_df[mask]['reconstruction_error'],\n",
    "               c=colors[label], label=label, alpha=0.7, s=30)\n",
    "\n",
    "# Add threshold line (we'll calculate this)\n",
    "threshold = np.percentile(normal_errors, 95)  # 95th percentile of normal errors\n",
    "ax2.axhline(y=threshold, color='orange', linestyle='--', linewidth=2, \n",
    "           label=f'Threshold ({threshold:.4f})')\n",
    "\n",
    "ax2.set_title(' Anomaly Detection Results', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Sample ID')\n",
    "ax2.set_ylabel('Reconstruction Error (MSE)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate performance metrics\n",
    "predicted_anomalies = reconstruction_errors > threshold\n",
    "true_anomalies = y_labels == 'anomaly'\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\n Anomaly Detection Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Threshold (95th percentile of normal): {threshold:.6f}\")\n",
    "print(f\"\")\n",
    "print(\"Reconstruction Error Statistics:\")\n",
    "print(f\"Normal data - Mean: {normal_errors.mean():.6f}, Std: {normal_errors.std():.6f}\")\n",
    "print(f\"Anomaly data - Mean: {anomaly_errors.mean():.6f}, Std: {anomaly_errors.std():.6f}\")\n",
    "print(f\"Separation ratio: {anomaly_errors.mean() / normal_errors.mean():.2f}x\")\n",
    "print()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_anomalies, predicted_anomalies, \n",
    "                           target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_anomalies, predicted_anomalies)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"True       Normal  Anomaly\")\n",
    "print(f\"Normal     {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
    "print(f\"Anomaly    {cm[1,0]:6d}  {cm[1,1]:6d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what the autoencoder learned - feature reconstruction quality\n",
    "# Calculate reconstruction error per feature\n",
    "feature_errors = np.mean((X_test - X_reconstructed) ** 2, axis=0)\n",
    "\n",
    "# Create feature importance visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Feature reconstruction errors\n",
    "bars = ax1.bar(range(len(feature_names)), feature_errors, alpha=0.7, color='steelblue')\n",
    "ax1.set_title(' Feature Reconstruction Difficulty', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Sensor Features')\n",
    "ax1.set_ylabel('Average Reconstruction Error')\n",
    "ax1.set_xticks(range(len(feature_names)))\n",
    "ax1.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    if height > 0.01:  # Only show labels for significant errors\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Compare original vs reconstructed for a few key features\n",
    "sample_idx = 850  # Pick an anomalous sample\n",
    "key_features = ['temp_1', 'pressure_1', 'vibration_x', 'power_consumption', 'efficiency']\n",
    "key_indices = [feature_names.index(feat) for feat in key_features]\n",
    "\n",
    "original_values = X_test[sample_idx, key_indices]\n",
    "reconstructed_values = X_reconstructed[sample_idx, key_indices]\n",
    "\n",
    "x = np.arange(len(key_features))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, original_values, width, label='Original', alpha=0.8, color='green')\n",
    "ax2.bar(x + width/2, reconstructed_values, width, label='Reconstructed', alpha=0.8, color='red')\n",
    "\n",
    "ax2.set_title(f'Reconstruction Quality - Sample {sample_idx} ({y_labels[sample_idx]})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Key Features')\n",
    "ax2.set_ylabel('Standardized Values')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(key_features)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of autoencoder capabilities\n",
    "print(\"\\n Autoencoder Analysis Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Successfully compressed 20D sensor data to 8D bottleneck\")\n",
    "print(f\" Learned to reconstruct normal operating conditions well\")\n",
    "print(f\"Anomalies show {anomaly_errors.mean()/normal_errors.mean():.1f}x higher reconstruction error\")\n",
    "print(f\"Achieved {((predicted_anomalies == true_anomalies).mean()*100):.1f}% accuracy in anomaly detection\")\n",
    "print()\n",
    "print(\" Key Insights:\")\n",
    "most_difficult = feature_names[np.argmax(feature_errors)]\n",
    "easiest = feature_names[np.argmin(feature_errors)]\n",
    "print(f\"• Most difficult to reconstruct: {most_difficult} (complex patterns)\")\n",
    "print(f\"• Easiest to reconstruct: {easiest} (predictable patterns)\")\n",
    "print(f\"• Autoencoder learned compressed representation preserving anomaly detection capability\")\n",
    "print(f\"• This approach scales to much larger datasets and more complex patterns\")\n",
    "print()\n",
    "print(\" Real-world Applications:\")\n",
    "print(\"• Continuous monitoring of manufacturing equipment\")\n",
    "print(\"• Early detection of equipment failures before breakdown\")\n",
    "print(\"• Reduced maintenance costs through predictive maintenance\")\n",
    "print(\"• Quality control in production lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### 🎯 Key Takeaways from Unsupervised Learning\n",
    "\n",
    "Congratulations! You've now explored the fascinating world of **unsupervised learning algorithms**. Each algorithm we've covered offers unique strengths and is suited for different types of problems and data characteristics.\n",
    "\n",
    "### 📊 Algorithm Comparison Summary\n",
    "\n",
    "| Algorithm | Best For | Strengths | Main Limitations |\n",
    "|-----------|----------|-----------|------------------|\n",
    "| **K-Means** | Spherical clusters, large datasets | Fast, simple, scalable | Need to choose k, assumes spherical clusters |\n",
    "| **Hierarchical** | Small datasets, unknown cluster count | No k needed, creates dendrogram | Expensive O(n³), sensitive to outliers |\n",
    "| **DBSCAN** | Arbitrary shapes, outlier detection | Finds any shape, detects noise | Sensitive to parameters, struggles with varying densities |\n",
    "| **PCA** | Dimensionality reduction, visualization | Fast, preserves variance | Linear only, components hard to interpret |\n",
    "| **t-SNE** | Non-linear visualization | Beautiful plots, reveals patterns | Slow, only for visualization, non-deterministic |\n",
    "| **Autoencoders** | Complex patterns, anomaly detection | Handles non-linearity, flexible | Needs large data, computationally expensive |\n",
    "\n",
    "### 🛠️ Choosing the Right Algorithm\n",
    "\n",
    "**For Clustering:**\n",
    "- Start with **K-Means** for quick exploration\n",
    "- Use **Hierarchical** when you don't know the number of clusters\n",
    "- Choose **DBSCAN** for irregular shapes and outlier detection\n",
    "\n",
    "**For Dimensionality Reduction:**\n",
    "- Use **PCA** for linear relationships and interpretability\n",
    "- Apply **t-SNE** for stunning visualizations of complex data\n",
    "- Consider **Autoencoders** for non-linear patterns with large datasets\n",
    "\n",
    "### 🚀 Next Steps for Your Learning Journey\n",
    "\n",
    "1. **Practice with Real Data**: Apply these algorithms to datasets from your domain of interest\n",
    "2. **Experiment with Parameters**: Understanding hyperparameter tuning is crucial for success\n",
    "3. **Combine Algorithms**: Use PCA before clustering, or t-SNE after clustering for visualization\n",
    "4. **Learn Evaluation Metrics**: Study silhouette score, Davies-Bouldin index, and other clustering metrics\n",
    "5. **Explore Advanced Variants**: HDBSCAN, UMAP, Variational Autoencoders, and more\n",
    "\n",
    "### 🌟 The Power of Unsupervised Learning\n",
    "\n",
    "Remember that unsupervised learning is often the **first step** in understanding your data. It helps you:\n",
    "- Discover hidden patterns and structures\n",
    "- Prepare data for supervised learning\n",
    "- Generate insights for business decisions\n",
    "- Create better features for machine learning models\n",
    "\n",
    "### 💡 Final Advice\n",
    "\n",
    "**\"The best algorithm is the one you understand well and can explain to others.\"** \n",
    "\n",
    "Start simple, understand the fundamentals, and gradually move to more complex methods. Always validate your results and remember that domain expertise is just as important as algorithmic knowledge.\n",
    "\n",
    "Good luck on your machine learning journey! 🚀✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
