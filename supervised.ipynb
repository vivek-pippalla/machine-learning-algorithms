{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Complete Guide to Supervised Learning Algorithms\n",
        "\n",
        "Welcome to your comprehensive journey through supervised learning! This notebook will take you from beginner to intermediate understanding of the most important supervised learning algorithms used in machine learning today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 What is Supervised Learning?\n",
        "\n",
        "**Supervised learning** is like learning with a teacher who provides you with both questions and correct answers. The algorithm learns from labeled training data (input-output pairs) to make predictions on new, unseen data.\n",
        "\n",
        "### 🌟 Key Characteristics:\n",
        "- **Labeled Data**: We have both input features (X) and target outputs (y)\n",
        "- **Learning Goal**: Find a function f(X) that maps inputs to correct outputs\n",
        "- **Two Main Types**:\n",
        "  - **Regression**: Predicting continuous values (house prices, temperature)\n",
        "  - **Classification**: Predicting categories (spam/not spam, dog/cat)\n",
        "\n",
        "### 🔍 Real-World Examples:\n",
        "- **Email Spam Detection**: Learning from thousands of emails labeled as \"spam\" or \"not spam\"\n",
        "- **Medical Diagnosis**: Using patient data and known diagnoses to predict diseases\n",
        "- **Stock Price Prediction**: Using historical market data to forecast future prices\n",
        "- **Image Recognition**: Learning from millions of labeled photos to identify objects\n",
        "\n",
        "### 🎯 Why is Supervised Learning Important?\n",
        "- **High Accuracy**: When you have good labeled data, supervised learning often achieves excellent performance\n",
        "- **Wide Applicability**: Works across many domains and problem types\n",
        "- **Business Value**: Directly solves many real-world prediction problems that companies face\n",
        "- **Foundation**: Understanding supervised learning is crucial for mastering machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's import the essential libraries we'll use throughout this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification, make_regression, load_iris, load_boston\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"🎉 All libraries imported successfully!\")\n",
        "print(\"Ready to explore supervised learning algorithms!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 Linear Regression\n",
        "\n",
        "### 🧠 Intuitive Explanation\n",
        "\n",
        "Think of Linear Regression as **drawing the best straight line through a cloud of points**. Imagine you're a real estate agent trying to predict house prices based on size. You plot house sizes on the x-axis and prices on the y-axis, then draw a line that gets as close as possible to all the points. This line becomes your \"crystal ball\" for predicting future house prices!\n",
        "\n",
        "**Simple Analogy**: It's like finding the \"average trend\" in your data - if houses generally get more expensive as they get bigger, linear regression finds the exact mathematical relationship.\n",
        "\n",
        "### ⚙️ How It Works (Mechanism)\n",
        "\n",
        "Linear Regression finds the best-fitting line by:\n",
        "\n",
        "1. **The Mathematical Model**: `y = mx + b` (or `y = β₁x + β₀` in ML terms)\n",
        "   - `y` = predicted value (house price)\n",
        "   - `x` = input feature (house size)\n",
        "   - `m` (or β₁) = slope (how much price increases per square foot)\n",
        "   - `b` (or β₀) = y-intercept (base price when size = 0)\n",
        "\n",
        "2. **Loss Function**: Uses **Mean Squared Error (MSE)**\n",
        "   - MSE = (1/n) × Σ(actual - predicted)²\n",
        "   - Squares the errors to penalize large mistakes more heavily\n",
        "\n",
        "3. **Optimization**: Uses **Gradient Descent** or **Normal Equation**\n",
        "   - Gradient Descent: Iteratively adjusts the line to minimize error\n",
        "   - Normal Equation: Calculates optimal parameters directly using calculus\n",
        "\n",
        "4. **Multiple Features**: For multiple inputs: `y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ`\n",
        "   - Now we're fitting a plane (3D) or hyperplane (>3D) instead of a line\n",
        "\n",
        "### 📝 Pseudo Structure / Workflow\n",
        "\n",
        "```\n",
        "1. TRAINING PHASE:\n",
        "   - Load labeled data (X_features, y_target)\n",
        "   - Split into train/test sets\n",
        "   - Initialize parameters (slope=0, intercept=0)\n",
        "   - For each iteration:\n",
        "     * Make predictions: y_pred = X * slope + intercept\n",
        "     * Calculate error: MSE = mean((y_true - y_pred)²)\n",
        "     * Update parameters using gradient descent\n",
        "   - Stop when error stops improving\n",
        "\n",
        "2. PREDICTION PHASE:\n",
        "   - Use learned parameters: y_new = X_new * slope + intercept\n",
        "   - Return continuous numerical predictions\n",
        "\n",
        "3. EVALUATION PHASE:\n",
        "   - Calculate MSE, R² score, Mean Absolute Error\n",
        "   - Plot actual vs predicted values\n",
        "```\n",
        "\n",
        "### ✅ Use Cases\n",
        "\n",
        "- **Real Estate**: Predicting house prices based on size, location, age\n",
        "- **Sales Forecasting**: Predicting sales based on advertising spend, seasonality\n",
        "- **Medical**: Predicting drug dosage based on patient weight, age\n",
        "- **Finance**: Estimating stock returns based on market indicators\n",
        "- **Manufacturing**: Predicting production costs based on materials, labor\n",
        "- **Marketing**: Estimating customer lifetime value based on behavior metrics\n",
        "- **Sports**: Predicting player performance based on training metrics\n",
        "\n",
        "### 💡 Why & When To Use\n",
        "\n",
        "**✅ Strengths:**\n",
        "- **Fast and Simple**: Extremely quick to train and predict (O(n) time complexity)\n",
        "- **Interpretable**: You can easily understand what each feature contributes\n",
        "- **No Hyperparameters**: Works out-of-the-box with minimal tuning\n",
        "- **Baseline Model**: Great starting point for any regression problem\n",
        "- **Probabilistic**: Provides confidence intervals for predictions\n",
        "\n",
        "**❌ Limitations:**\n",
        "- **Linear Relationships Only**: Can't capture curved or complex patterns\n",
        "- **Sensitive to Outliers**: A few extreme points can skew the entire line\n",
        "- **Feature Scaling Matters**: Works best when features are on similar scales\n",
        "- **Assumes Independence**: Features shouldn't be highly correlated\n",
        "\n",
        "**🎯 When to Use:**\n",
        "- When you need quick results and interpretability is important\n",
        "- As a baseline before trying complex algorithms\n",
        "- When relationships appear roughly linear\n",
        "- For small to medium datasets\n",
        "- When you need to explain your model to stakeholders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💻 Code Example\n",
        "\n",
        "> **Problem**: Let's predict house prices based on house size using Linear Regression. We'll create a synthetic dataset where house prices generally increase with size, then build a model to learn this relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Create synthetic house price data\n",
        "np.random.seed(42)\n",
        "house_sizes = np.random.normal(2000, 500, 100).reshape(-1, 1)  # House sizes (sq ft)\n",
        "house_prices = 100 * house_sizes.flatten() + np.random.normal(0, 10000, 100) + 50000  # Price = 100*size + noise + base\n",
        "\n",
        "print(f\" Dataset Info:\")\n",
        "print(f\"Number of houses: {len(house_sizes)}\")\n",
        "print(f\"Average house size: {house_sizes.mean():.0f} sq ft\")\n",
        "print(f\"Average house price: ${house_prices.mean():,.0f}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(house_sizes, house_prices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n Model Performance:\")\n",
        "print(f\"R² Score: {r2:.3f} (closer to 1 is better)\")\n",
        "print(f\"Mean Absolute Error: ${mae:,.0f}\")\n",
        "print(f\"Root Mean Squared Error: ${np.sqrt(mse):,.0f}\")\n",
        "\n",
        "print(f\"\\n Model Equation:\")\n",
        "print(f\"Price = ${model.coef_[0]:.2f} × Size + ${model.intercept_:,.0f}\")\n",
        "print(f\"Interpretation: Each additional sq ft adds ${model.coef_[0]:.2f} to the price\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Regression Line\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_train, y_train, alpha=0.6, color='blue', label='Training Data')\n",
        "plt.scatter(X_test, y_test, alpha=0.8, color='red', label='Test Data')\n",
        "# Create line for visualization\n",
        "X_line = np.linspace(house_sizes.min(), house_sizes.max(), 100).reshape(-1, 1)\n",
        "y_line = model.predict(X_line)\n",
        "plt.plot(X_line, y_line, color='green', linewidth=2, label='Regression Line')\n",
        "plt.xlabel('House Size (sq ft)')\n",
        "plt.ylabel('House Price ($)')\n",
        "plt.title(' Linear Regression: House Size vs Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Actual vs Predicted\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(y_test, y_pred, alpha=0.7, color='purple')\n",
        "# Perfect prediction line\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual Prices ($)')\n",
        "plt.ylabel('Predicted Prices ($)')\n",
        "plt.title(f' Actual vs Predicted\\n(R² = {r2:.3f})')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Residuals\n",
        "plt.subplot(1, 3, 3)\n",
        "residuals = y_test - y_pred\n",
        "plt.scatter(y_pred, residuals, alpha=0.7, color='orange')\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Predicted Prices ($)')\n",
        "plt.ylabel('Residuals ($)')\n",
        "plt.title(' Residual Plot\\n(Should be random around 0)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Example predictions\n",
        "print(f\"\\n🔮 Example Predictions:\")\n",
        "example_sizes = np.array([[1500], [2000], [2500]])\n",
        "example_predictions = model.predict(example_sizes)\n",
        "for size, price in zip(example_sizes.flatten(), example_predictions):\n",
        "    print(f\"   {size} sq ft house → Predicted price: ${price:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Logistic Regression\n",
        "\n",
        "### 🧠 Intuitive Explanation\n",
        "\n",
        "Think of Logistic Regression as a **\"smart switch\"** that decides between two options. Unlike Linear Regression which draws a straight line, Logistic Regression creates an **S-shaped curve** that smoothly transitions from 0 to 1.\n",
        "\n",
        "**Perfect Analogy**: Imagine you're a doctor deciding if a patient has a disease (1) or not (0) based on their symptoms. As symptoms get worse, the probability smoothly increases from 0% to 100% - it doesn't jump suddenly. The S-curve captures this gradual transition.\n",
        "\n",
        "**Key Insight**: Linear Regression asks \"how much?\", while Logistic Regression asks \"what's the probability?\" or \"which category?\"\n",
        "\n",
        "### ⚙️ How It Works (Mechanism)\n",
        "\n",
        "Logistic Regression uses the **Sigmoid Function** to convert any real number into a probability between 0 and 1:\n",
        "\n",
        "1. **The Sigmoid Function**: \n",
        "   - σ(z) = 1 / (1 + e^(-z))\n",
        "   - Where z = β₀ + β₁x₁ + β₂x₂ + ... (linear combination)\n",
        "   - Output is always between 0 and 1 (perfect for probabilities!)\n",
        "\n",
        "2. **Decision Boundary**:\n",
        "   - When z = 0, σ(z) = 0.5 (the decision threshold)\n",
        "   - If probability > 0.5 → Predict Class 1\n",
        "   - If probability < 0.5 → Predict Class 0\n",
        "\n",
        "3. **Loss Function**: **Log-Likelihood** (not MSE!)\n",
        "   - Penalizes confident wrong predictions more heavily\n",
        "   - For binary: -[y×log(p) + (1-y)×log(1-p)]\n",
        "\n",
        "4. **Optimization**: Uses **Gradient Descent**\n",
        "   - No closed-form solution like Linear Regression\n",
        "   - Iteratively finds optimal parameters\n",
        "\n",
        "5. **Odds and Log-Odds**:\n",
        "   - Odds = p/(1-p) (ratio of success to failure)\n",
        "   - Log-odds = ln(p/(1-p)) = z (the linear part!)\n",
        "\n",
        "### 📝 Pseudo Structure / Workflow\n",
        "\n",
        "```\n",
        "1. TRAINING PHASE:\n",
        "   - Load labeled data (X_features, y_binary_labels)\n",
        "   - Split into train/test sets\n",
        "   - Initialize parameters (weights, bias)\n",
        "   - For each iteration:\n",
        "     * Calculate z = X * weights + bias\n",
        "     * Apply sigmoid: probabilities = 1/(1 + exp(-z))\n",
        "     * Calculate log-likelihood loss\n",
        "     * Update parameters using gradient descent\n",
        "   - Stop when loss converges\n",
        "\n",
        "2. PREDICTION PHASE:\n",
        "   - Calculate probabilities: p = sigmoid(X_new * weights + bias)\n",
        "   - Apply threshold: class = 1 if p > 0.5 else 0\n",
        "   - Return both probabilities and class predictions\n",
        "\n",
        "3. EVALUATION PHASE:\n",
        "   - Calculate accuracy, precision, recall, F1-score\n",
        "   - Plot confusion matrix and ROC curve\n",
        "```\n",
        "\n",
        "### ✅ Use Cases\n",
        "\n",
        "- **Medical Diagnosis**: Disease/no disease based on symptoms and test results\n",
        "- **Email Spam Detection**: Spam/not spam based on email content and metadata\n",
        "- **Marketing**: Will customer buy/not buy based on demographics and behavior\n",
        "- **Finance**: Loan approval (approve/reject) based on credit history\n",
        "- **Quality Control**: Product defective/good based on manufacturing parameters\n",
        "- **Web Analytics**: User will click/not click on advertisement\n",
        "- **HR**: Employee will stay/leave company based on satisfaction metrics\n",
        "- **Sports**: Team will win/lose based on player statistics\n",
        "\n",
        "### 💡 Why & When To Use\n",
        "\n",
        "**✅ Strengths:**\n",
        "- **Probabilistic Output**: Gives probability estimates, not just class predictions\n",
        "- **No Assumptions**: Doesn't assume normal distribution of features\n",
        "- **Fast and Efficient**: Quick training and prediction (O(n) complexity)\n",
        "- **Interpretable**: Coefficients show feature importance and direction\n",
        "- **Robust**: Less sensitive to outliers than Linear Regression\n",
        "- **No Hyperparameters**: Works well with default settings\n",
        "\n",
        "**❌ Limitations:**\n",
        "- **Linear Decision Boundary**: Can only create straight-line separations\n",
        "- **Binary Focus**: Originally designed for binary classification\n",
        "- **Sensitive to Feature Scale**: Works better with standardized features\n",
        "- **Large Sample Size**: Needs sufficient data for stable results\n",
        "- **Independence Assumption**: Features should not be highly correlated\n",
        "\n",
        "**🎯 When to Use:**\n",
        "- When you need probability estimates (not just yes/no predictions)\n",
        "- For binary classification problems\n",
        "- When interpretability is important\n",
        "- As a baseline model before trying complex algorithms\n",
        "- When data is roughly linearly separable\n",
        "- For real-time applications (fast prediction needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💻 Code Example\n",
        "\n",
        "> **Problem**: Let's build a spam email classifier using Logistic Regression. We'll create a synthetic dataset with email features (word counts, sender reputation, etc.) and predict whether an email is spam or not spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create synthetic email spam dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Features: [suspicious_words, sender_reputation, email_length, has_links, urgency_words]\n",
        "X, y = make_classification(n_samples=n_samples, n_features=5, n_redundant=0, \n",
        "                          n_informative=5, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Give meaningful names to our features\n",
        "feature_names = ['Suspicious Words', 'Sender Reputation', 'Email Length', 'Has Links', 'Urgency Words']\n",
        "email_df = pd.DataFrame(X, columns=feature_names)\n",
        "email_df['Is Spam'] = y\n",
        "\n",
        "print(f\" Email Dataset Info:\")\n",
        "print(f\"Total emails: {len(email_df)}\")\n",
        "print(f\"Spam emails: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
        "print(f\"Not spam emails: {len(y) - sum(y)} ({(len(y) - sum(y))/len(y)*100:.1f}%)\")\n",
        "print(f\"\\n Feature statistics:\")\n",
        "print(email_df.describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features (important for logistic regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability of spam\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\" Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"AUC Score: {auc_score:.3f} (closer to 1 is better)\")\n",
        "print(f\"\\n Detailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Spam', 'Spam']))\n",
        "\n",
        "# Show feature importance (coefficients)\n",
        "print(f\"\\n Feature Importance (Coefficients):\")\n",
        "for name, coef in zip(feature_names, model.coef_[0]):\n",
        "    direction = \" Increases\" if coef > 0 else \" Decreases\"\n",
        "    print(f\"  {name}: {coef:.3f} ({direction} spam probability)\")\n",
        "\n",
        "print(f\"\\nModel Equation (simplified):\")\n",
        "print(f\"log-odds = {model.intercept_[0]:.3f} + ... (linear combination of features)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Confusion Matrix\n",
        "plt.subplot(2, 3, 1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])\n",
        "plt.title(' Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Plot 2: ROC Curve\n",
        "plt.subplot(2, 3, 2)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(' ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Feature Coefficients\n",
        "plt.subplot(2, 3, 3)\n",
        "colors = ['red' if coef < 0 else 'green' for coef in model.coef_[0]]\n",
        "plt.barh(feature_names, model.coef_[0], color=colors, alpha=0.7)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title(' Feature Importance\\n(Positive = More Spam)')\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Probability Distribution\n",
        "plt.subplot(2, 3, 4)\n",
        "spam_probs = y_pred_proba[y_test == 1]\n",
        "not_spam_probs = y_pred_proba[y_test == 0]\n",
        "plt.hist(not_spam_probs, alpha=0.7, label='Not Spam', color='blue', bins=20)\n",
        "plt.hist(spam_probs, alpha=0.7, label='Spam', color='red', bins=20)\n",
        "plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Count')\n",
        "plt.title(' Probability Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Sigmoid Function\n",
        "plt.subplot(2, 3, 5)\n",
        "z = np.linspace(-6, 6, 100)\n",
        "sigmoid = 1 / (1 + np.exp(-z))\n",
        "plt.plot(z, sigmoid, 'b-', linewidth=2)\n",
        "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Decision Threshold')\n",
        "plt.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "plt.xlabel('z (Linear Combination)')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(' Sigmoid Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Prediction Examples\n",
        "plt.subplot(2, 3, 6)\n",
        "sample_indices = np.random.choice(len(X_test), 20, replace=False)\n",
        "sample_probs = y_pred_proba[sample_indices]\n",
        "sample_actual = y_test[sample_indices]\n",
        "colors = ['green' if actual == pred else 'red' \n",
        "          for actual, pred in zip(sample_actual, (sample_probs > 0.5).astype(int))]\n",
        "plt.scatter(range(len(sample_probs)), sample_probs, c=colors, alpha=0.7, s=50)\n",
        "plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Spam Probability')\n",
        "plt.title('🔮 Sample Predictions\\n(Green=Correct, Red=Wrong)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example predictions with interpretations\n",
        "print(f\"🔮 Example Email Classifications:\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Get some test examples\n",
        "example_indices = [0, 50, 100, 150, 199]  # Pick diverse examples\n",
        "\n",
        "for i, idx in enumerate(example_indices):\n",
        "    actual = y_test[idx]\n",
        "    prob = y_pred_proba[idx]\n",
        "    predicted = 1 if prob > 0.5 else 0\n",
        "    \n",
        "    print(f\"\\n Email #{i+1}:\")\n",
        "    print(f\"   Features: {X_test_scaled[idx].round(2)}\")\n",
        "    print(f\"   Spam Probability: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "    print(f\"   Predicted: {' SPAM' if predicted == 1 else ' NOT SPAM'}\")\n",
        "    print(f\"   Actual: {' SPAM' if actual == 1 else ' NOT SPAM'}\")\n",
        "    print(f\"   Result: {' CORRECT' if predicted == actual else ' WRONG'}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    if prob > 0.8:\n",
        "        confidence = \"Very High\"\n",
        "    elif prob > 0.6:\n",
        "        confidence = \"High\"\n",
        "    elif prob > 0.4:\n",
        "        confidence = \"Uncertain\"\n",
        "    elif prob > 0.2:\n",
        "        confidence = \"Low\"\n",
        "    else:\n",
        "        confidence = \"Very Low\"\n",
        "    \n",
        "    print(f\"   Confidence: {confidence} spam likelihood\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌳 Decision Trees\n",
        "\n",
        "### 🧠 Intuitive Explanation\n",
        "\n",
        "Think of Decision Trees as playing a **sophisticated game of \"20 Questions\"** to make decisions. Just like how you might ask \"Is it bigger than a breadbox?\" to guess an object, Decision Trees ask a series of yes/no questions about your data to reach a conclusion.\n",
        "\n",
        "**Perfect Analogy**: Imagine you're a doctor diagnosing patients:\n",
        "- \"Is fever > 100°F?\" → If YES: \"Is cough present?\" → If YES: \"Likely flu\"\n",
        "- \"Is fever > 100°F?\" → If NO: \"Is headache severe?\" → If YES: \"Likely migraine\"\n",
        "\n",
        "Each question splits patients into groups, and you keep asking questions until each group has mostly the same diagnosis. That's exactly how Decision Trees work!\n",
        "\n",
        "**Key Insight**: Decision Trees create **rectangular decision boundaries** - they divide the data space into boxes where each box gets one prediction.\n",
        "\n",
        "### ⚙️ How It Works (Mechanism)\n",
        "\n",
        "Decision Trees build themselves by repeatedly asking \"What's the best question to ask?\"\n",
        "\n",
        "1. **Tree Structure**:\n",
        "   - **Root Node**: The first question (top of the tree)\n",
        "   - **Internal Nodes**: Subsequent questions (decision points)\n",
        "   - **Leaves**: Final predictions (end points)\n",
        "   - **Branches**: Paths from questions to answers\n",
        "\n",
        "2. **Splitting Criteria** - How to choose the best question:\n",
        "   - **Classification**: Use **Gini Impurity** or **Entropy**\n",
        "     - Gini: 1 - Σ(probability of class i)²\n",
        "     - Entropy: -Σ(p × log₂(p)) for each class\n",
        "     - Lower values = purer groups = better splits\n",
        "   - **Regression**: Use **Mean Squared Error** or **Mean Absolute Error**\n",
        "\n",
        "3. **Information Gain**: Measures how much a split improves purity\n",
        "   - Information Gain = Impurity(parent) - Weighted Average(Impurity(children))\n",
        "   - Choose the split with highest information gain\n",
        "\n",
        "4. **Stopping Criteria**:\n",
        "   - Maximum depth reached\n",
        "   - Minimum samples per leaf\n",
        "   - No more useful splits possible\n",
        "   - Perfect purity achieved\n",
        "\n",
        "5. **Prediction Process**:\n",
        "   - Start at root node\n",
        "   - Follow the path based on feature values\n",
        "   - Stop at leaf node and return its prediction\n",
        "\n",
        "### 📝 Pseudo Structure / Workflow\n",
        "\n",
        "```\n",
        "1. TRAINING PHASE (Recursive Tree Building):\n",
        "   - Start with all training data at root\n",
        "   - For each possible split (feature + threshold):\n",
        "     * Calculate information gain\n",
        "   - Choose split with highest information gain\n",
        "   - Split data into left and right child nodes\n",
        "   - Repeat recursively for each child until stopping criteria met\n",
        "   - At each leaf, store the majority class (classification) or mean value (regression)\n",
        "\n",
        "2. PREDICTION PHASE:\n",
        "   - For new sample:\n",
        "     * Start at root node\n",
        "     * If feature_value <= threshold: go left, else go right\n",
        "     * Repeat until reaching a leaf\n",
        "     * Return leaf's prediction\n",
        "\n",
        "3. EVALUATION PHASE:\n",
        "   - Calculate accuracy/MSE on test set\n",
        "   - Visualize tree structure\n",
        "   - Analyze feature importance\n",
        "```\n",
        "\n",
        "### ✅ Use Cases\n",
        "\n",
        "- **Medical Diagnosis**: Symptom-based disease identification\n",
        "- **Credit Approval**: Loan decisions based on financial history\n",
        "- **Customer Segmentation**: Grouping customers by behavior patterns\n",
        "- **Marketing**: Predicting customer response to campaigns\n",
        "- **Quality Control**: Detecting defective products in manufacturing\n",
        "- **HR**: Employee performance evaluation and promotion decisions\n",
        "- **Fraud Detection**: Identifying suspicious transactions\n",
        "- **Game AI**: Creating decision-making logic for NPCs\n",
        "- **Recommendation Systems**: Content filtering and suggestions\n",
        "\n",
        "### 💡 Why & When To Use\n",
        "\n",
        "**✅ Strengths:**\n",
        "- **Highly Interpretable**: You can literally see the decision process\n",
        "- **No Feature Scaling**: Works with raw data (doesn't care about units)\n",
        "- **Handles Mixed Data**: Works with both numerical and categorical features\n",
        "- **Non-linear Patterns**: Can capture complex interactions and curves\n",
        "- **Feature Selection**: Automatically ignores irrelevant features\n",
        "- **Fast Prediction**: O(log n) prediction time\n",
        "- **Handles Missing Values**: Can work around missing data\n",
        "\n",
        "**❌ Limitations:**\n",
        "- **Overfitting**: Can memorize training data (high variance)\n",
        "- **Instability**: Small data changes can create very different trees\n",
        "- **Bias**: Tends to favor features with more levels\n",
        "- **Linear Boundaries**: Each split is axis-aligned (can't handle diagonal patterns)\n",
        "- **Class Imbalance**: May be biased toward majority classes\n",
        "- **Limited Smoothness**: Creates step functions, not smooth curves\n",
        "\n",
        "**🎯 When to Use:**\n",
        "- When interpretability is crucial (medical, legal, financial decisions)\n",
        "- With mixed data types (numbers + categories)\n",
        "- When you have non-linear relationships\n",
        "- For rule extraction and knowledge discovery\n",
        "- When features don't need scaling\n",
        "- As a baseline before trying ensemble methods\n",
        "- For educational purposes (easy to understand and visualize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💻 Code Example\n",
        "\n",
        "> **Problem**: Let's build a loan approval system using Decision Trees. We'll create a dataset with customer features (income, credit score, age, etc.) and predict whether to approve or deny a loan application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.tree import export_text\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Create synthetic loan approval dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate realistic loan application features\n",
        "income = np.random.lognormal(mean=10.5, sigma=0.8, size=n_samples)  # Income ($30k - $200k+)\n",
        "credit_score = np.random.normal(loc=650, scale=100, size=n_samples)  # Credit score (300-850)\n",
        "credit_score = np.clip(credit_score, 300, 850)\n",
        "age = np.random.normal(loc=40, scale=12, size=n_samples)  # Age\n",
        "age = np.clip(age, 18, 80)\n",
        "loan_amount = np.random.normal(loc=200000, scale=100000, size=n_samples)  # Loan amount\n",
        "loan_amount = np.clip(loan_amount, 50000, 800000)\n",
        "employment_years = np.random.exponential(scale=5, size=n_samples)  # Years employed\n",
        "employment_years = np.clip(employment_years, 0, 40)\n",
        "\n",
        "# Create the target: loan approval based on realistic criteria\n",
        "# Higher income, better credit score, reasonable debt-to-income ratio → more likely approval\n",
        "debt_to_income = loan_amount / income\n",
        "approval_score = (0.3 * (credit_score - 300) / 550 +  # Normalized credit score\n",
        "                 0.3 * np.log(income) / np.log(200000) +  # Log income factor\n",
        "                 0.2 * employment_years / 20 +  # Employment stability\n",
        "                 0.2 * (1 / (1 + debt_to_income)) +  # Debt-to-income ratio\n",
        "                 np.random.normal(0, 0.1, n_samples))  # Some randomness\n",
        "\n",
        "# Convert to binary approval (1 = approve, 0 = deny)\n",
        "loan_approved = (approval_score > 0.5).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "X = np.column_stack([income, credit_score, age, loan_amount, employment_years])\n",
        "feature_names = ['Income', 'Credit_Score', 'Age', 'Loan_Amount', 'Employment_Years']\n",
        "loan_df = pd.DataFrame(X, columns=feature_names)\n",
        "loan_df['Approved'] = loan_approved\n",
        "\n",
        "print(f\" Loan Dataset Info:\")\n",
        "print(f\"Total applications: {len(loan_df)}\")\n",
        "print(f\"Approved: {sum(loan_approved)} ({sum(loan_approved)/len(loan_approved)*100:.1f}%)\")\n",
        "print(f\"Denied: {len(loan_approved) - sum(loan_approved)} ({(len(loan_approved) - sum(loan_approved))/len(loan_approved)*100:.1f}%)\")\n",
        "print(f\"\\n Feature statistics:\")\n",
        "print(loan_df.describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, loan_approved, test_size=0.2, \n",
        "                                                    random_state=42, stratify=loan_approved)\n",
        "\n",
        "# Create and train the Decision Tree\n",
        "# We'll limit depth to avoid overfitting and make it interpretable\n",
        "model = DecisionTreeClassifier(\n",
        "    max_depth=4,        # Limit depth for interpretability\n",
        "    min_samples_split=50,  # Minimum samples to create a split\n",
        "    min_samples_leaf=20,   # Minimum samples in each leaf\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of approval\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\" Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Tree Depth: {model.get_depth()}\")\n",
        "print(f\"Number of Leaves: {model.get_n_leaves()}\")\n",
        "print(f\"\\n Detailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Denied', 'Approved']))\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\n Feature Importance:\")\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "for _, row in feature_importance_df.iterrows():\n",
        "    print(f\"  {row['Feature']}: {row['Importance']:.3f} ({row['Importance']*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the Decision Tree\n",
        "plt.figure(figsize=(20, 12))\n",
        "plot_tree(model, \n",
        "         feature_names=feature_names,\n",
        "         class_names=['Denied', 'Approved'],\n",
        "         filled=True,\n",
        "         rounded=True,\n",
        "         fontsize=10)\n",
        "plt.title(' Loan Approval Decision Tree\\n(Each box shows: condition, samples, value, class)', \n",
        "         fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Decision Tree Rules (Text Format):\")\n",
        "print(\"=\" * 80)\n",
        "tree_rules = export_text(model, feature_names=feature_names)\n",
        "print(tree_rules[:1500] + \"\\n... (truncated for readability)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional visualizations\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Confusion Matrix\n",
        "plt.subplot(2, 3, 1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
        "            xticklabels=['Denied', 'Approved'], yticklabels=['Denied', 'Approved'])\n",
        "plt.title(' Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Plot 2: Feature Importance\n",
        "plt.subplot(2, 3, 2)\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(feature_names)))\n",
        "plt.barh(feature_names, importances, color=colors, alpha=0.8)\n",
        "plt.xlabel('Importance')\n",
        "plt.title(' Feature Importance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Tree Depth vs Accuracy (to show overfitting)\n",
        "plt.subplot(2, 3, 3)\n",
        "depths = range(1, 11)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for depth in depths:\n",
        "    temp_model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    temp_model.fit(X_train, y_train)\n",
        "    train_scores.append(temp_model.score(X_train, y_train))\n",
        "    test_scores.append(temp_model.score(X_test, y_test))\n",
        "\n",
        "plt.plot(depths, train_scores, 'o-', label='Training Accuracy', color='blue')\n",
        "plt.plot(depths, test_scores, 'o-', label='Test Accuracy', color='red')\n",
        "plt.axvline(x=4, color='green', linestyle='--', alpha=0.7, label='Our Model')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(' Model Complexity vs Performance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Decision Boundary (2D projection)\n",
        "plt.subplot(2, 3, 4)\n",
        "# Use two most important features for 2D visualization\n",
        "top_features = feature_importance_df.head(2)['Feature'].values\n",
        "feat1_idx = feature_names.index(top_features[0])\n",
        "feat2_idx = feature_names.index(top_features[1])\n",
        "\n",
        "# Create 2D decision tree for visualization\n",
        "model_2d = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "X_2d = X_train[:, [feat1_idx, feat2_idx]]\n",
        "model_2d.fit(X_2d, y_train)\n",
        "\n",
        "# Create mesh for decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
        "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h*1000),\n",
        "                     np.arange(y_min, y_max, h*10))\n",
        "Z = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_train, cmap=plt.cm.RdYlBu, alpha=0.6)\n",
        "plt.xlabel(top_features[0])\n",
        "plt.ylabel(top_features[1])\n",
        "plt.title(' Decision Boundary\\n(Rectangular regions)')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "# Plot 5: Probability distribution\n",
        "plt.subplot(2, 3, 5)\n",
        "approved_probs = y_pred_proba[y_test == 1]\n",
        "denied_probs = y_pred_proba[y_test == 0]\n",
        "plt.hist(denied_probs, alpha=0.7, label='Actually Denied', color='red', bins=15)\n",
        "plt.hist(approved_probs, alpha=0.7, label='Actually Approved', color='blue', bins=15)\n",
        "plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
        "plt.xlabel('Predicted Probability of Approval')\n",
        "plt.ylabel('Count')\n",
        "plt.title(' Prediction Probabilities')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Sample predictions\n",
        "plt.subplot(2, 3, 6)\n",
        "sample_indices = np.random.choice(len(X_test), 20, replace=False)\n",
        "sample_probs = y_pred_proba[sample_indices]\n",
        "sample_actual = y_test[sample_indices]\n",
        "sample_pred = y_pred[sample_indices]\n",
        "colors = ['green' if actual == pred else 'red' \n",
        "          for actual, pred in zip(sample_actual, sample_pred)]\n",
        "plt.scatter(range(len(sample_probs)), sample_probs, c=colors, alpha=0.7, s=50)\n",
        "plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Approval Probability')\n",
        "plt.title(' Sample Predictions\\n(Green=Correct, Red=Wrong)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example loan applications with explanations\n",
        "print(f\" Example Loan Application Decisions:\")\n",
        "print(f\"{'='*100}\")\n",
        "\n",
        "# Create some example applications\n",
        "example_applications = np.array([\n",
        "    [80000, 750, 35, 250000, 8],    # High income, good credit\n",
        "    [35000, 600, 25, 300000, 2],    # Low income, poor credit, high loan\n",
        "    [120000, 700, 45, 200000, 15],  # High income, good credit, experienced\n",
        "    [50000, 550, 30, 400000, 1],    # Medium income, poor credit, very high loan\n",
        "    [90000, 680, 40, 150000, 10]    # Good overall profile\n",
        "])\n",
        "\n",
        "example_predictions = model.predict(example_applications)\n",
        "example_probabilities = model.predict_proba(example_applications)[:, 1]\n",
        "\n",
        "for i, (app, pred, prob) in enumerate(zip(example_applications, example_predictions, example_probabilities)):\n",
        "    print(f\"\\n Application #{i+1}:\")\n",
        "    print(f\"   Income: ${app[0]:,.0f}\")\n",
        "    print(f\"   Credit Score: {app[1]:.0f}\")\n",
        "    print(f\"   Age: {app[2]:.0f} years\")\n",
        "    print(f\"   Loan Amount: ${app[3]:,.0f}\")\n",
        "    print(f\"   Employment Years: {app[4]:.0f}\")\n",
        "    print(f\"   Debt-to-Income Ratio: {app[3]/app[0]:.2f}\")\n",
        "    \n",
        "    decision = \" APPROVED\" if pred == 1 else \"❌ DENIED\"\n",
        "    confidence = \"High\" if abs(prob - 0.5) > 0.3 else \"Medium\" if abs(prob - 0.5) > 0.1 else \"Low\"\n",
        "    \n",
        "    print(f\"   Decision: {decision}\")\n",
        "    print(f\"   Approval Probability: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "    print(f\"   Confidence: {confidence}\")\n",
        "    \n",
        "    # Simple explanation based on tree logic\n",
        "    if app[1] > 650:  # Good credit score\n",
        "        if app[3]/app[0] < 4:  # Reasonable debt-to-income\n",
        "            print(f\"    Key factors: Good credit score + reasonable debt-to-income ratio\")\n",
        "        else:\n",
        "            print(f\"    Key factors: Good credit score but high debt-to-income ratio\")\n",
        "    else:  # Poor credit score\n",
        "        print(f\"    Key factors: Credit score below 650 is a major risk factor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌲 Random Forests\n",
        "\n",
        "### 🧠 Intuitive Explanation\n",
        "\n",
        "Imagine you're trying to decide which movie to watch, but instead of asking just one friend, you ask **100 different friends** for their opinion and then go with the **majority vote**. That's exactly how Random Forests work!\n",
        "\n",
        "**Perfect Analogy**: Random Forests are like assembling a **\"committee of experts\"** (decision trees) where:\n",
        "- Each expert (tree) sees slightly different information\n",
        "- Each expert makes their own decision\n",
        "- The final decision is made by majority vote (classification) or average (regression)\n",
        "- The crowd is usually smarter than any individual expert!\n",
        "\n",
        "**Key Insight**: Instead of putting all your trust in one decision tree (which might overfit), Random Forests combine many trees to get a more robust and accurate prediction.\n",
        "\n",
        "### ⚙️ How It Works (Mechanism)\n",
        "\n",
        "Random Forests introduce **two levels of randomness** to create diverse trees:\n",
        "\n",
        "1. **Bootstrap Sampling (Bagging)**:\n",
        "   - Each tree trains on a different random sample of the data\n",
        "   - Sample WITH replacement (some data points appear multiple times)\n",
        "   - This creates different \"perspectives\" for each tree\n",
        "\n",
        "2. **Random Feature Selection**:\n",
        "   - At each split, only consider a random subset of features\n",
        "   - Typically √(total features) for classification\n",
        "   - Typically (total features)/3 for regression\n",
        "   - Prevents trees from all making the same splits\n",
        "\n",
        "3. **Tree Building Process**:\n",
        "   - Build each tree to full depth (no pruning usually)\n",
        "   - Each tree sees ~63% of original data (due to bootstrap sampling)\n",
        "   - Trees are trained independently (can be parallelized!)\n",
        "\n",
        "4. **Prediction Process**:\n",
        "   - **Classification**: Each tree votes for a class → majority wins\n",
        "   - **Regression**: Each tree predicts a value → take the average\n",
        "   - Can also get probability estimates by counting votes\n",
        "\n",
        "5. **Out-of-Bag (OOB) Error**:\n",
        "   - Each tree can be tested on ~37% of data it never saw\n",
        "   - Provides internal validation without separate test set\n",
        "   - Useful for hyperparameter tuning\n",
        "\n",
        "### 📝 Pseudo Structure / Workflow\n",
        "\n",
        "```\n",
        "1. TRAINING PHASE:\n",
        "   - For each of N trees:\n",
        "     * Create bootstrap sample (sample with replacement)\n",
        "     * Build decision tree using random feature subsets at each split\n",
        "     * Grow tree to full depth (no pruning)\n",
        "   - Store all N trained trees\n",
        "\n",
        "2. PREDICTION PHASE:\n",
        "   - For new sample:\n",
        "     * Pass through all N trees to get N predictions\n",
        "     * Classification: majority vote among predictions\n",
        "     * Regression: average of all predictions\n",
        "   - Return final ensemble prediction\n",
        "\n",
        "3. EVALUATION PHASE:\n",
        "   - Calculate performance on test set\n",
        "   - Analyze feature importance (averaged across trees)\n",
        "   - Use OOB error for internal validation\n",
        "```\n",
        "\n",
        "### ✅ Use Cases\n",
        "\n",
        "- **Bioinformatics**: Gene expression analysis, drug discovery\n",
        "- **Finance**: Credit risk assessment, algorithmic trading\n",
        "- **E-commerce**: Product recommendation, price optimization\n",
        "- **Healthcare**: Disease prediction, treatment effectiveness\n",
        "- **Marketing**: Customer churn prediction, campaign optimization\n",
        "- **Image Recognition**: Object detection, facial recognition\n",
        "- **Environmental Science**: Climate modeling, species classification\n",
        "- **Manufacturing**: Quality control, predictive maintenance\n",
        "- **Real Estate**: Property valuation, market analysis\n",
        "- **Sports Analytics**: Player performance, game outcome prediction\n",
        "\n",
        "### 💡 Why & When To Use\n",
        "\n",
        "**✅ Strengths:**\n",
        "- **Excellent Performance**: Often achieves high accuracy out-of-the-box\n",
        "- **Reduces Overfitting**: Ensemble averaging smooths out individual tree mistakes\n",
        "- **Handles Missing Values**: Can work around missing data points\n",
        "- **Feature Importance**: Provides robust feature ranking\n",
        "- **No Feature Scaling**: Works with raw data\n",
        "- **Parallelizable**: Trees can be trained simultaneously\n",
        "- **OOB Validation**: Built-in cross-validation mechanism\n",
        "- **Robust**: Less sensitive to outliers than single trees\n",
        "- **Mixed Data Types**: Handles numerical and categorical features\n",
        "\n",
        "**❌ Limitations:**\n",
        "- **Less Interpretable**: Can't easily visualize 100+ trees\n",
        "- **Memory Intensive**: Stores many full trees in memory\n",
        "- **Can Still Overfit**: With very noisy data or too many trees\n",
        "- **Biased to Categorical**: May favor features with more categories\n",
        "- **Slower Prediction**: Must query many trees (though still fast)\n",
        "- **Black Box**: Harder to explain individual predictions\n",
        "- **Hyperparameter Sensitive**: Performance depends on tuning n_estimators, max_features, etc.\n",
        "\n",
        "**🎯 When to Use:**\n",
        "- When you need high accuracy without much tuning\n",
        "- For tabular data with mixed feature types\n",
        "- When single decision trees are overfitting\n",
        "- For feature selection and importance ranking\n",
        "- When you have sufficient computational resources\n",
        "- As a strong baseline before trying complex algorithms\n",
        "- In competitions and real-world applications where performance matters most"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💻 Code Example\n",
        "\n",
        "> **Problem**: Let's build a comprehensive customer churn prediction system using Random Forests. We'll create a telecom dataset with customer features and predict whether customers will cancel their service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import validation_curve\n",
        "import time\n",
        "\n",
        "# Create synthetic customer churn dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 2000\n",
        "\n",
        "# Generate realistic customer features\n",
        "monthly_charges = np.random.normal(loc=65, scale=25, size=n_samples)  # Monthly bill\n",
        "monthly_charges = np.clip(monthly_charges, 20, 150)\n",
        "\n",
        "tenure_months = np.random.exponential(scale=20, size=n_samples)  # How long they've been customers\n",
        "tenure_months = np.clip(tenure_months, 1, 72)\n",
        "\n",
        "total_charges = monthly_charges * tenure_months + np.random.normal(0, 100, n_samples)\n",
        "total_charges = np.clip(total_charges, 100, 10000)\n",
        "\n",
        "age = np.random.normal(loc=45, scale=15, size=n_samples)\n",
        "age = np.clip(age, 18, 80)\n",
        "\n",
        "num_services = np.random.poisson(lam=3, size=n_samples)  # Number of services used\n",
        "num_services = np.clip(num_services, 1, 8)\n",
        "\n",
        "support_calls = np.random.poisson(lam=2, size=n_samples)  # Customer service calls\n",
        "contract_length = np.random.choice([1, 12, 24], size=n_samples, p=[0.4, 0.3, 0.3])  # Contract length\n",
        "\n",
        "# Create churn target based on realistic factors\n",
        "# Higher charges, shorter tenure, more support calls → higher churn probability\n",
        "churn_score = (0.2 * (monthly_charges - 40) / 60 +  # Normalized monthly charges\n",
        "               0.3 * (1 / (tenure_months + 1)) +    # Inverse tenure (new customers churn more)\n",
        "               0.2 * support_calls / 10 +           # Support calls factor\n",
        "               0.1 * (1 / contract_length) +        # Shorter contract → higher churn\n",
        "               0.1 * (1 / num_services) +           # Fewer services → higher churn\n",
        "               0.1 * np.random.normal(0, 0.5, n_samples))  # Random factor\n",
        "\n",
        "# Convert to binary churn (1 = will churn, 0 = will stay)\n",
        "customer_churn = (churn_score > 0.4).astype(int)\n",
        "\n",
        "# Create feature matrix\n",
        "X = np.column_stack([monthly_charges, tenure_months, total_charges, age, \n",
        "                     num_services, support_calls, contract_length])\n",
        "feature_names = ['Monthly_Charges', 'Tenure_Months', 'Total_Charges', 'Age', \n",
        "                'Num_Services', 'Support_Calls', 'Contract_Length']\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "churn_df = pd.DataFrame(X, columns=feature_names)\n",
        "churn_df['Churn'] = customer_churn\n",
        "\n",
        "print(f\" Customer Churn Dataset Info:\")\n",
        "print(f\"Total customers: {len(churn_df)}\")\n",
        "print(f\"Will churn: {sum(customer_churn)} ({sum(customer_churn)/len(customer_churn)*100:.1f}%)\")\n",
        "print(f\"Will stay: {len(customer_churn) - sum(customer_churn)} ({(len(customer_churn) - sum(customer_churn))/len(customer_churn)*100:.1f}%)\")\n",
        "print(f\"\\n Feature statistics:\")\n",
        "print(churn_df.describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, customer_churn, test_size=0.2, \n",
        "                                                    random_state=42, stratify=customer_churn)\n",
        "\n",
        "# Train a single Decision Tree for comparison\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest with different numbers of trees\n",
        "print(\" Training Random Forest...\")\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,        # Number of trees\n",
        "    max_depth=10,           # Limit depth to prevent overfitting\n",
        "    min_samples_split=10,   # Minimum samples to split\n",
        "    min_samples_leaf=5,     # Minimum samples per leaf\n",
        "    max_features='sqrt',    # Number of features per split\n",
        "    bootstrap=True,         # Use bootstrap sampling\n",
        "    oob_score=True,         # Calculate out-of-bag score\n",
        "    random_state=42,\n",
        "    n_jobs=-1               # Use all CPU cores\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# Make predictions\n",
        "single_pred = single_tree.predict(X_test)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compare performance\n",
        "single_accuracy = accuracy_score(y_test, single_pred)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "oob_score = rf_model.oob_score_\n",
        "\n",
        "print(f\"\\n Model Comparison:\")\n",
        "print(f\"Single Decision Tree Accuracy: {single_accuracy:.3f}\")\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.3f}\")\n",
        "print(f\"Improvement: {rf_accuracy - single_accuracy:.3f} ({(rf_accuracy/single_accuracy-1)*100:.1f}% better)\")\n",
        "print(f\"Out-of-Bag Score: {oob_score:.3f}\")\n",
        "\n",
        "print(f\"\\n Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_pred, target_names=['Stay', 'Churn']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis\n",
        "importances = rf_model.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
        "\n",
        "# Create feature importance DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances,\n",
        "    'Std': std\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\n Feature Importance Ranking:\")\n",
        "for _, row in feature_importance_df.iterrows():\n",
        "    print(f\"  {row['Feature']}: {row['Importance']:.3f} (±{row['Std']:.3f})\")\n",
        "\n",
        "# Model details\n",
        "print(f\"\\n Random Forest Details:\")\n",
        "print(f\"Number of trees: {rf_model.n_estimators}\")\n",
        "print(f\"Max features per split: {rf_model.max_features}\")\n",
        "print(f\"Average tree depth: {np.mean([tree.get_depth() for tree in rf_model.estimators_]):.1f}\")\n",
        "print(f\"Average leaves per tree: {np.mean([tree.get_n_leaves() for tree in rf_model.estimators_]):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualizations\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Feature Importance with Error Bars\n",
        "plt.subplot(2, 4, 1)\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(feature_names)))\n",
        "y_pos = np.arange(len(feature_names))\n",
        "plt.barh(y_pos, feature_importance_df['Importance'], \n",
        "         xerr=feature_importance_df['Std'], color=colors, alpha=0.8, capsize=3)\n",
        "plt.yticks(y_pos, feature_importance_df['Feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title(' Feature Importance\\n(with standard deviation)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Number of Trees vs Accuracy\n",
        "plt.subplot(2, 4, 2)\n",
        "n_trees = range(10, 201, 20)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "oob_scores = []\n",
        "\n",
        "for n in n_trees:\n",
        "    temp_rf = RandomForestClassifier(n_estimators=n, random_state=42, oob_score=True)\n",
        "    temp_rf.fit(X_train, y_train)\n",
        "    train_scores.append(temp_rf.score(X_train, y_train))\n",
        "    test_scores.append(temp_rf.score(X_test, y_test))\n",
        "    oob_scores.append(temp_rf.oob_score_)\n",
        "\n",
        "plt.plot(n_trees, train_scores, 'o-', label='Training', color='blue', alpha=0.7)\n",
        "plt.plot(n_trees, test_scores, 'o-', label='Test', color='red', alpha=0.7)\n",
        "plt.plot(n_trees, oob_scores, 'o-', label='OOB', color='green', alpha=0.7)\n",
        "plt.axvline(x=100, color='purple', linestyle='--', alpha=0.7, label='Our Model')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(' Trees vs Performance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Confusion Matrix\n",
        "plt.subplot(2, 4, 3)\n",
        "cm = confusion_matrix(y_test, rf_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Stay', 'Churn'], yticklabels=['Stay', 'Churn'])\n",
        "plt.title(' Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Plot 4: ROC Curve\n",
        "plt.subplot(2, 4, 4)\n",
        "fpr, tpr, _ = roc_curve(y_test, rf_pred_proba)\n",
        "auc_score = roc_auc_score(y_test, rf_pred_proba)\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f'Random Forest (AUC = {auc_score:.3f})')\n",
        "\n",
        "# Compare with single tree\n",
        "single_proba = single_tree.predict_proba(X_test)[:, 1]\n",
        "fpr_single, tpr_single, _ = roc_curve(y_test, single_proba)\n",
        "auc_single = roc_auc_score(y_test, single_proba)\n",
        "plt.plot(fpr_single, tpr_single, linewidth=2, alpha=0.7, \n",
        "         label=f'Single Tree (AUC = {auc_single:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(' ROC Curves Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Prediction Probability Distribution\n",
        "plt.subplot(2, 4, 5)\n",
        "stay_probs = rf_pred_proba[y_test == 0]\n",
        "churn_probs = rf_pred_proba[y_test == 1]\n",
        "plt.hist(stay_probs, alpha=0.7, label='Actually Stay', color='blue', bins=20)\n",
        "plt.hist(churn_probs, alpha=0.7, label='Actually Churn', color='red', bins=20)\n",
        "plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
        "plt.xlabel('Churn Probability')\n",
        "plt.ylabel('Count')\n",
        "plt.title(' Probability Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Individual Tree Performance Variation\n",
        "plt.subplot(2, 4, 6)\n",
        "tree_accuracies = []\n",
        "for tree in rf_model.estimators_:\n",
        "    tree_pred = tree.predict(X_test)\n",
        "    tree_acc = accuracy_score(y_test, tree_pred)\n",
        "    tree_accuracies.append(tree_acc)\n",
        "\n",
        "plt.hist(tree_accuracies, bins=20, alpha=0.7, color='green')\n",
        "plt.axvline(x=rf_accuracy, color='red', linestyle='--', linewidth=2, \n",
        "           label=f'Ensemble: {rf_accuracy:.3f}')\n",
        "plt.axvline(x=np.mean(tree_accuracies), color='blue', linestyle='--', linewidth=2,\n",
        "           label=f'Mean Tree: {np.mean(tree_accuracies):.3f}')\n",
        "plt.xlabel('Individual Tree Accuracy')\n",
        "plt.ylabel('Count')\n",
        "plt.title(' Individual Tree Performance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 7: Learning Curve (Sample Size vs Performance)\n",
        "plt.subplot(2, 4, 7)\n",
        "sample_sizes = np.linspace(100, len(X_train), 10).astype(int)\n",
        "train_scores_lc = []\n",
        "test_scores_lc = []\n",
        "\n",
        "for size in sample_sizes:\n",
        "    temp_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    temp_rf.fit(X_train[:size], y_train[:size])\n",
        "    train_scores_lc.append(temp_rf.score(X_train[:size], y_train[:size]))\n",
        "    test_scores_lc.append(temp_rf.score(X_test, y_test))\n",
        "\n",
        "plt.plot(sample_sizes, train_scores_lc, 'o-', label='Training', color='blue')\n",
        "plt.plot(sample_sizes, test_scores_lc, 'o-', label='Test', color='red')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(' Learning Curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 8: Feature Correlation with Target\n",
        "plt.subplot(2, 4, 8)\n",
        "correlations = []\n",
        "for i, feature in enumerate(feature_names):\n",
        "    corr = np.corrcoef(X[:, i], customer_churn)[0, 1]\n",
        "    correlations.append(abs(corr))\n",
        "\n",
        "colors = ['red' if corr < 0 else 'green' for corr in correlations]\n",
        "plt.barh(feature_names, correlations, color=colors, alpha=0.7)\n",
        "plt.xlabel('Absolute Correlation with Churn')\n",
        "plt.title(' Feature-Target Correlations')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed customer churn predictions with explanations\n",
        "print(f\" Customer Churn Predictions with Explanations:\")\n",
        "print(f\"{'='*120}\")\n",
        "\n",
        "# Select diverse examples\n",
        "example_indices = [10, 50, 100, 200, 350]  # Different customer profiles\n",
        "\n",
        "for i, idx in enumerate(example_indices):\n",
        "    actual = y_test[idx]\n",
        "    prob = rf_pred_proba[idx]\n",
        "    predicted = rf_pred[idx]\n",
        "    \n",
        "    print(f\"\\n👤 Customer #{i+1} Profile:\")\n",
        "    print(f\"   Monthly Charges: ${X_test[idx, 0]:.2f}\")\n",
        "    print(f\"   Tenure: {X_test[idx, 1]:.0f} months\")\n",
        "    print(f\"   Total Charges: ${X_test[idx, 2]:.2f}\")\n",
        "    print(f\"   Age: {X_test[idx, 3]:.0f} years\")\n",
        "    print(f\"   Number of Services: {X_test[idx, 4]:.0f}\")\n",
        "    print(f\"   Support Calls: {X_test[idx, 5]:.0f}\")\n",
        "    print(f\"   Contract Length: {X_test[idx, 6]:.0f} months\")\n",
        "    \n",
        "    # Risk assessment\n",
        "    risk_level = \"Very High\" if prob > 0.8 else \"High\" if prob > 0.6 else \"Medium\" if prob > 0.4 else \"Low\" if prob > 0.2 else \"Very Low\"\n",
        "    prediction_text = \" WILL CHURN\" if predicted == 1 else \" WILL STAY\"\n",
        "    actual_text = \"ACTUALLY CHURNED\" if actual == 1 else \" ACTUALLY STAYED\"\n",
        "    correct = \" CORRECT\" if predicted == actual else \" WRONG\"\n",
        "    \n",
        "    print(f\"   Churn Probability: {prob:.3f} ({prob*100:.1f}%)\")\n",
        "    print(f\"   Risk Level: {risk_level}\")\n",
        "    print(f\"   Prediction: {prediction_text}\")\n",
        "    print(f\"   Actual Outcome: {actual_text}\")\n",
        "    print(f\"   Model Performance: {correct}\")\n",
        "    \n",
        "    # Key risk factors (simplified interpretation)\n",
        "    risk_factors = []\n",
        "    if X_test[idx, 0] > 80:  # High monthly charges\n",
        "        risk_factors.append(\"High monthly charges ($80+)\")\n",
        "    if X_test[idx, 1] < 12:  # New customer\n",
        "        risk_factors.append(\"New customer (<12 months)\")\n",
        "    if X_test[idx, 5] > 3:   # Many support calls\n",
        "        risk_factors.append(\"Frequent support calls (>3)\")\n",
        "    if X_test[idx, 4] < 2:   # Few services\n",
        "        risk_factors.append(\"Limited service usage (<2 services)\")\n",
        "    if X_test[idx, 6] == 1:  # Month-to-month\n",
        "        risk_factors.append(\"Month-to-month contract\")\n",
        "    \n",
        "    if risk_factors:\n",
        "        print(f\"    Key Risk Factors: {', '.join(risk_factors)}\")\n",
        "    else:\n",
        "        print(f\"     Low Risk Profile: Stable customer characteristics\")\n",
        "\n",
        "# Overall model insights\n",
        "print(f\"\\n\\n Random Forest Model Insights:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\" Model Performance: {rf_accuracy:.1%} accuracy on test set\")\n",
        "print(f\" Ensemble Power: {rf_accuracy - single_accuracy:+.3f} improvement over single tree\")\n",
        "print(f\" Out-of-Bag Validation: {oob_score:.3f} (internal cross-validation)\")\n",
        "print(f\" Most Important Features: {', '.join(feature_importance_df.head(3)['Feature'].values)}\")\n",
        "print(f\" Training Time: {training_time:.2f} seconds for 100 trees\")\n",
        "print(f\" Model Complexity: ~{np.mean([tree.get_n_leaves() for tree in rf_model.estimators_]):.0f} avg leaves per tree\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Support Vector Machines (SVM)\n",
        "\n",
        "### 🧠 Intuitive Explanation\n",
        "\n",
        "Imagine you're trying to separate two groups of people at a party - **introverts** and **extroverts**. Instead of drawing any random line between them, SVM finds the **\"widest possible hallway\"** that separates the groups, keeping equal distance from the closest people on both sides.\n",
        "\n",
        "**Perfect Analogy**: SVM is like a **\"security guard with maximum personal space\"**:\n",
        "- It finds the boundary that stays as far as possible from both groups\n",
        "- Only the people closest to the boundary (\"support vectors\") matter for drawing the line\n",
        "- It can even work in crowded spaces by using a \"magic lens\" (kernel) to see hidden patterns\n",
        "\n",
        "**Key Insight**: While other algorithms try to minimize errors, SVM maximizes the **margin** - the safety zone around the decision boundary. This makes it incredibly robust and generalizable!\n",
        "\n",
        "### ⚙️ How It Works (Mechanism)\n",
        "\n",
        "SVM is based on finding the **optimal separating hyperplane** with maximum margin:\n",
        "\n",
        "1. **The Margin Concept**:\n",
        "   - **Margin**: The distance between the decision boundary and closest data points\n",
        "   - **Support Vectors**: The data points closest to the boundary (these define everything!)\n",
        "   - **Goal**: Maximize margin = maximize generalization ability\n",
        "\n",
        "2. **Linear SVM** (Separable Case):\n",
        "   - Find hyperplane: w·x + b = 0\n",
        "   - Maximize margin: 2/||w|| (where ||w|| is the norm of weight vector)\n",
        "   - Minimize: ||w||²/2 subject to: yᵢ(w·xᵢ + b) ≥ 1 for all points\n",
        "   - This is a **quadratic optimization problem**\n",
        "\n",
        "3. **Soft Margin SVM** (Non-separable Case):\n",
        "   - Introduces \"slack variables\" ξᵢ to allow some misclassification\n",
        "   - Minimize: ||w||²/2 + C∑ξᵢ\n",
        "   - **C parameter**: Trade-off between margin size and training accuracy\n",
        "   - High C = Hard margin (less tolerant of errors)\n",
        "   - Low C = Soft margin (more tolerant of errors)\n",
        "\n",
        "4. **Kernel Trick** (Non-linear Cases):\n",
        "   - **Problem**: Real data is rarely linearly separable\n",
        "   - **Solution**: Map data to higher dimensional space where it becomes separable\n",
        "   - **Magic**: We never explicitly compute the mapping, just the kernel function!\n",
        "   - **Popular Kernels**:\n",
        "     - Linear: K(x,y) = x·y\n",
        "     - Polynomial: K(x,y) = (γx·y + r)ᵈ\n",
        "     - RBF (Gaussian): K(x,y) = exp(-γ||x-y||²)\n",
        "     - Sigmoid: K(x,y) = tanh(γx·y + r)\n",
        "\n",
        "5. **Decision Function**:\n",
        "   - f(x) = sign(∑αᵢyᵢK(xᵢ,x) + b)\n",
        "   - Only support vectors (αᵢ > 0) contribute to the decision\n",
        "   - Distance from boundary gives confidence measure\n",
        "\n",
        "### 📝 Pseudo Structure / Workflow\n",
        "\n",
        "```\n",
        "1. TRAINING PHASE:\n",
        "   - Scale/normalize features (very important for SVM!)\n",
        "   - Choose kernel (linear, RBF, polynomial) and hyperparameters (C, γ)\n",
        "   - Solve quadratic optimization problem:\n",
        "     * Find support vectors (closest points to boundary)\n",
        "     * Calculate optimal weights and bias\n",
        "     * Store only support vectors (memory efficient!)\n",
        "\n",
        "2. PREDICTION PHASE:\n",
        "   - For new point x:\n",
        "     * Calculate decision function using support vectors\n",
        "     * f(x) = ∑(αᵢ × yᵢ × kernel(support_vectorᵢ, x)) + bias\n",
        "     * Return sign(f(x)) for classification\n",
        "     * Return |f(x)| for confidence measure\n",
        "\n",
        "3. EVALUATION PHASE:\n",
        "   - Test on validation set\n",
        "   - Analyze support vectors (fewer = better generalization)\n",
        "   - Tune hyperparameters using cross-validation\n",
        "```\n",
        "\n",
        "### ✅ Use Cases\n",
        "\n",
        "- **Text Classification**: Spam detection, sentiment analysis, document categorization\n",
        "- **Image Recognition**: Facial recognition, object detection, medical imaging\n",
        "- **Bioinformatics**: Gene classification, protein structure prediction\n",
        "- **Finance**: Credit risk assessment, fraud detection, algorithmic trading\n",
        "- **Medical Diagnosis**: Cancer detection, drug discovery, treatment prediction\n",
        "- **Computer Vision**: Image segmentation, pattern recognition\n",
        "- **Web Search**: Ranking algorithms, recommendation systems\n",
        "- **Speech Recognition**: Voice command classification\n",
        "- **Marketing**: Customer segmentation, targeted advertising\n",
        "- **Quality Control**: Defect detection in manufacturing\n",
        "\n",
        "### 💡 Why & When To Use\n",
        "\n",
        "**✅ Strengths:**\n",
        "- **Excellent Generalization**: Maximum margin principle reduces overfitting\n",
        "- **Memory Efficient**: Stores only support vectors (often much smaller than dataset)\n",
        "- **Kernel Power**: Can handle non-linear patterns through kernel trick\n",
        "- **High-Dimensional Data**: Performs well even when features >> samples\n",
        "- **Robust to Outliers**: Focus on boundary points makes it less sensitive to distant outliers\n",
        "- **Theoretical Foundation**: Strong mathematical backing and guarantees\n",
        "- **Versatile**: Works for classification, regression, and outlier detection\n",
        "\n",
        "**❌ Limitations:**\n",
        "- **Slow Training**: O(n²) to O(n³) complexity for large datasets\n",
        "- **Feature Scaling Critical**: Very sensitive to feature scales\n",
        "- **No Probabilistic Output**: Gives distance from boundary, not probabilities\n",
        "- **Hyperparameter Sensitivity**: Performance heavily depends on C and γ tuning\n",
        "- **Black Box**: Kernel transformations make interpretation difficult\n",
        "- **Memory Usage**: Can be memory-intensive with complex kernels\n",
        "- **Binary Focus**: Originally designed for binary classification\n",
        "\n",
        "**🎯 When to Use:**\n",
        "- For high-dimensional data (text, genomics, images)\n",
        "- When you have more features than samples\n",
        "- For non-linear classification problems (use RBF kernel)\n",
        "- When generalization is more important than training speed\n",
        "- For small to medium-sized datasets (< 10,000 samples)\n",
        "- When data is not too noisy\n",
        "- For binary classification problems\n",
        "- When you need a theoretically sound algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💻 Code Example\n",
        "\n",
        "> **Problem**: Let's build a robust image classification system using SVM. We'll create a dataset simulating pixel features from different types of images and classify them into categories. We'll also explore different kernels and their effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV, validation_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a more complex dataset for SVM demonstration\n",
        "np.random.seed(42)\n",
        "n_samples = 1500\n",
        "\n",
        "# Create synthetic image classification dataset\n",
        "# Simulating features like average brightness, contrast, edge density, color variance, etc.\n",
        "X, y = make_classification(\n",
        "    n_samples=n_samples,\n",
        "    n_features=20,        # High-dimensional feature space (good for SVM)\n",
        "    n_informative=15,     # 15 useful features\n",
        "    n_redundant=3,        # 3 redundant features\n",
        "    n_clusters_per_class=2,  # Multiple clusters per class (non-linear)\n",
        "    class_sep=0.8,        # Moderate class separation\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create meaningful feature names\n",
        "feature_names = [f'Feature_{i+1}' for i in range(20)]\n",
        "class_names = ['Natural_Images', 'Artificial_Images']\n",
        "\n",
        "print(f\" Image Classification Dataset Info:\")\n",
        "print(f\"Total images: {len(X)}\")\n",
        "print(f\"Features per image: {X.shape[1]}\")\n",
        "print(f\"Natural images: {sum(y == 0)} ({sum(y == 0)/len(y)*100:.1f}%)\")\n",
        "print(f\"Artificial images: {sum(y == 1)} ({sum(y == 1)/len(y)*100:.1f}%)\")\n",
        "print(f\"\\n Feature statistics (first 5 features):\")\n",
        "feature_df = pd.DataFrame(X[:, :5], columns=feature_names[:5])\n",
        "print(feature_df.describe().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# Feature scaling (CRITICAL for SVM!)\n",
        "print(\"🔧 Scaling features (essential for SVM)...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Before scaling - Feature 1: mean={X_train[:, 0].mean():.3f}, std={X_train[:, 0].std():.3f}\")\n",
        "print(f\"After scaling - Feature 1: mean={X_train_scaled[:, 0].mean():.3f}, std={X_train_scaled[:, 0].std():.3f}\")\n",
        "\n",
        "# Train different SVM models with different kernels\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "svm_models = {}\n",
        "svm_scores = {}\n",
        "svm_times = {}\n",
        "\n",
        "print(f\"\\n🎯 Training SVM with different kernels...\")\n",
        "for kernel in kernels:\n",
        "    print(f\"  Training {kernel} SVM...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if kernel == 'linear':\n",
        "        model = SVC(kernel=kernel, C=1.0, random_state=42)\n",
        "    elif kernel == 'poly':\n",
        "        model = SVC(kernel=kernel, C=1.0, degree=3, random_state=42)\n",
        "    elif kernel == 'rbf':\n",
        "        model = SVC(kernel=kernel, C=1.0, gamma='scale', random_state=42)\n",
        "    else:  # sigmoid\n",
        "        model = SVC(kernel=kernel, C=1.0, gamma='scale', random_state=42)\n",
        "    \n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    test_score = model.score(X_test_scaled, y_test)\n",
        "    \n",
        "    svm_models[kernel] = model\n",
        "    svm_scores[kernel] = test_score\n",
        "    svm_times[kernel] = training_time\n",
        "    \n",
        "    print(f\"    Accuracy: {test_score:.3f}, Time: {training_time:.2f}s, Support Vectors: {len(model.support_)}\")\n",
        "\n",
        "# Find best kernel\n",
        "best_kernel = max(svm_scores, key=svm_scores.get)\n",
        "best_model = svm_models[best_kernel]\n",
        "\n",
        "print(f\"\\n Best Kernel: {best_kernel} (Accuracy: {svm_scores[best_kernel]:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for the best kernel\n",
        "print(f\"\\n Hyperparameter tuning for {best_kernel} SVM...\")\n",
        "\n",
        "# Define parameter grid\n",
        "if best_kernel == 'linear':\n",
        "    param_grid = {'C': [0.1, 1, 10, 100]}\n",
        "elif best_kernel == 'rbf':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
        "    }\n",
        "elif best_kernel == 'poly':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'degree': [2, 3, 4],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "else:  # sigmoid\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]\n",
        "    }\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    SVC(kernel=best_kernel, random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "tuned_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Cross-validation score: {grid_search.best_score_:.3f}\")\n",
        "print(f\"Test set accuracy: {tuned_model.score(X_test_scaled, y_test):.3f}\")\n",
        "\n",
        "# Make predictions with the tuned model\n",
        "y_pred = tuned_model.predict(X_test_scaled)\n",
        "\n",
        "# Model analysis\n",
        "print(f\"\\n Tuned SVM Model Analysis:\")\n",
        "print(f\"Number of support vectors: {len(tuned_model.support_)}\")\n",
        "print(f\"Support vector ratio: {len(tuned_model.support_)/len(X_train_scaled)*100:.1f}% of training data\")\n",
        "print(f\"Support vectors per class: {tuned_model.n_support_}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\n Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2b5fdf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualizations\n",
        "plt.figure(figsize=(20, 15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3706b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Kernel Comparison\n",
        "plt.subplot(3, 4, 1)\n",
        "kernels_list = list(svm_scores.keys())\n",
        "scores_list = list(svm_scores.values())\n",
        "colors = plt.cm.Set2(np.linspace(0, 1, len(kernels_list)))\n",
        "bars = plt.bar(kernels_list, scores_list, color=colors, alpha=0.8)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(' Kernel Comparison')\n",
        "plt.ylim(0.7, 1.0)\n",
        "for bar, score in zip(bars, scores_list):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{score:.3f}', ha='center', va='bottom')\n",
        "plt.grid(True, alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d8fc69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 2: Training Time vs Accuracy\n",
        "plt.subplot(3, 4, 2)\n",
        "times_list = list(svm_times.values())\n",
        "plt.scatter(times_list, scores_list, c=colors, s=100, alpha=0.8)\n",
        "for i, kernel in enumerate(kernels_list):\n",
        "    plt.annotate(kernel, (times_list[i], scores_list[i]), \n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "plt.xlabel('Training Time (seconds)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('⏱Training Time vs Accuracy')\n",
        "plt.grid(True, alpha=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54cc722",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 3: Confusion Matrix\n",
        "plt.subplot(3, 4, 3)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(' Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b13485",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 4: Decision Function Histogram\n",
        "plt.subplot(3, 4, 4)\n",
        "decision_scores = tuned_model.decision_function(X_test_scaled)\n",
        "class_0_scores = decision_scores[y_test == 0]\n",
        "class_1_scores = decision_scores[y_test == 1]\n",
        "plt.hist(class_0_scores, alpha=0.7, label=class_names[0], color='blue', bins=20)\n",
        "plt.hist(class_1_scores, alpha=0.7, label=class_names[1], color='red', bins=20)\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
        "plt.xlabel('Decision Function Value')\n",
        "plt.ylabel('Count')\n",
        "plt.title(' Decision Function Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084ee189",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5: C Parameter Effect (if RBF kernel)\n",
        "plt.subplot(3, 4, 5)\n",
        "if best_kernel == 'rbf':\n",
        "    C_range = np.logspace(-3, 3, 13)\n",
        "    train_scores, val_scores = validation_curve(\n",
        "        SVC(kernel='rbf', gamma='scale', random_state=42), \n",
        "        X_train_scaled, y_train, param_name='C', param_range=C_range, \n",
        "        cv=3, scoring='accuracy', n_jobs=-1\n",
        "    )\n",
        "    train_mean = train_scores.mean(axis=1)\n",
        "    val_mean = val_scores.mean(axis=1)\n",
        "    plt.semilogx(C_range, train_mean, 'o-', label='Training', color='blue')\n",
        "    plt.semilogx(C_range, val_mean, 'o-', label='Validation', color='red')\n",
        "    plt.axvline(x=tuned_model.C, color='green', linestyle='--', alpha=0.7, label='Best C')\n",
        "    plt.xlabel('C Parameter')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(' C Parameter Effect')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "    plt.text(0.5, 0.5, f'C Parameter Effect\\n(Only for RBF kernel)', \n",
        "             ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.title(' C Parameter Effect')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd57d8c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 6: Support Vectors Analysis\n",
        "plt.subplot(3, 4, 6)\n",
        "sv_per_class = tuned_model.n_support_\n",
        "plt.pie(sv_per_class, labels=class_names, autopct='%1.1f%%', startangle=90,\n",
        "        colors=['lightblue', 'lightcoral'])\n",
        "plt.title(f' Support Vectors Distribution\\n(Total: {sum(sv_per_class)})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c182ff5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 7: Feature Importance (for linear kernel) or 2D visualization\n",
        "plt.subplot(3, 4, 7)\n",
        "if best_kernel == 'linear':\n",
        "    # Linear SVM coefficients show feature importance\n",
        "    coef = tuned_model.coef_[0]\n",
        "    feature_importance = np.abs(coef)\n",
        "    top_features_idx = np.argsort(feature_importance)[-10:]  # Top 10 features\n",
        "    plt.barh(range(10), feature_importance[top_features_idx], alpha=0.8)\n",
        "    plt.yticks(range(10), [f'Feature_{i+1}' for i in top_features_idx])\n",
        "    plt.xlabel('Absolute Coefficient Value')\n",
        "    plt.title(' Feature Importance\\n(Linear SVM)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "    # 2D visualization using first two principal components\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_train_scaled)\n",
        "    \n",
        "    # Plot data points\n",
        "    plt.scatter(X_pca[y_train==0, 0], X_pca[y_train==0, 1], \n",
        "               c='blue', alpha=0.6, label=class_names[0])\n",
        "    plt.scatter(X_pca[y_train==1, 0], X_pca[y_train==1, 1], \n",
        "               c='red', alpha=0.6, label=class_names[1])\n",
        "    \n",
        "    # Highlight support vectors\n",
        "    sv_indices = tuned_model.support_\n",
        "    plt.scatter(X_pca[sv_indices, 0], X_pca[sv_indices, 1], \n",
        "               s=100, c='yellow', edgecolors='black', alpha=0.8, \n",
        "               label='Support Vectors')\n",
        "    \n",
        "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)')\n",
        "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)')\n",
        "    plt.title(' Data Visualization\\n(PCA + Support Vectors)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 8: Model Complexity Analysis\n",
        "plt.subplot(3, 4, 8)\n",
        "complexities = []\n",
        "accuracies = []\n",
        "sv_counts = []\n",
        "\n",
        "for kernel in kernels:\n",
        "    model = svm_models[kernel]\n",
        "    accuracies.append(svm_scores[kernel])\n",
        "    sv_counts.append(len(model.support_))\n",
        "    \n",
        "plt.scatter(sv_counts, accuracies, c=colors, s=100, alpha=0.8)\n",
        "for i, kernel in enumerate(kernels):\n",
        "    plt.annotate(kernel, (sv_counts[i], accuracies[i]), \n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "plt.xlabel('Number of Support Vectors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(' Model Complexity vs Performance')\n",
        "plt.grid(True, alpha=0.3)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
